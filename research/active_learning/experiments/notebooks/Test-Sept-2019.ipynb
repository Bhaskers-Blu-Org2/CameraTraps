{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, getpass, joblib, os, pickle, queue, random, sys, time, tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision.transforms import *\n",
    "\n",
    "sys.path.append('../..')\n",
    "from DL.utils import *\n",
    "from DL.networks import *\n",
    "from DL.losses import *\n",
    "from DL.Engine import Engine\n",
    "from DL.sqlite_data_loader import SQLDataLoader\n",
    "from Database.DB_models import *\n",
    "from sampling_methods.constants import get_AL_sampler\n",
    "from sampling_methods.constants import get_wrapper_AL_mapping\n",
    "get_wrapper_AL_mapping()\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# %matplotlib notebook\n",
    "# matplotlib.use('GTK')\n",
    "# import ipywidgets as wdg\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neural_network import MLPClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_output_dirname = './test_sept_2019_output'\n",
    "os.makedirs(notebook_output_dirname, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL = '/home/lynx/pretrainedmodels/embedding_triplet_resnet50_1499/triplet_resnet50_1499.tar'\n",
    "# EMBEDDING_MODEL = '/home/lynx/repos/CameraTraps/research/active_learning/experiments/notebooks/test_sept_2019_output/finetunedtriplet_resnet50_1000.tar'\n",
    "CROP_DIR = '/datadrive/missouricameratraps/crops_detector/'\n",
    "DB_LIMIT = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): NormalizedEmbeddingNet(\n",
       "    (inner_model): ResNet(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (relu): ReLU(inplace)\n",
       "        )\n",
       "      )\n",
       "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the pretrained embedding model\n",
    "checkpoint = load_checkpoint(EMBEDDING_MODEL)\n",
    "if checkpoint['loss_type'].lower() == 'center' or checkpoint['loss_type'].lower() == 'softmax':\n",
    "    embedding_net = SoftmaxNet(checkpoint['arch'], checkpoint['feat_dim'], False)\n",
    "else:\n",
    "    embedding_net = NormalizedEmbeddingNet(checkpoint['arch'], checkpoint['feat_dim'], False)\n",
    "model = torch.nn.DataParallel(embedding_net).cuda()\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter database name:········\n",
      "Enter username for database:········\n",
      "Enter password for database user:········\n"
     ]
    }
   ],
   "source": [
    "DB_NAME = getpass.getpass('Enter database name:')\n",
    "DB_USER = getpass.getpass('Enter username for database:')\n",
    "DB_PASSWORD = getpass.getpass('Enter password for database user:')\n",
    "RANDOM_SEED = 2345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveRecords(dataset, srcKind, destKind, rList):\n",
    "    for e in rList:\n",
    "        dataset.set_indices[srcKind].remove(e)\n",
    "        dataset.set_indices[destKind].append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_embedding(model, loss_type, train_dataset, P, K, epochs):\n",
    "    \"\"\"\n",
    "    Fine tune the embedding model.\n",
    "\n",
    "    Arguments:\n",
    "        model: Model to fine tune.\n",
    "        loss_type: The loss function to minimize while fine tuning.\n",
    "        train_dataset: Dataset object to use to train the embedding.\n",
    "        P: Number of classes to sample from the dataset if using a balanced loader.\n",
    "        K: Number of samples from each class to sample from the dataset if using a balanced loader.\n",
    "        epochs: Number of epochs to train the embedding for.\n",
    "    \"\"\"\n",
    "    train_dataset.image_mode()\n",
    "\n",
    "    if loss_type.lower() == 'softmax':\n",
    "        criterion = nn.CrossEntropyLoss().cuda()\n",
    "        train_loader = train_dataset.getSingleLoader()\n",
    "    elif loss_type.lower() == 'siamese':\n",
    "        criterion = OnlineContrastiveLoss(1, HardNegativePairSelector())\n",
    "        train_loader = train_dataset.getBalancedLoader(P = P, K = K)\n",
    "    else:\n",
    "        criterion = OnlineTripletLoss(1, RandomNegativeTripletSelector(1))\n",
    "        train_loader = train_dataset.getBalancedLoader(P = P, K = K)\n",
    "\n",
    "    params = model.parameters()\n",
    "    optimizer = torch.optim.Adam(params, lr = 0.0001)#, weight_decay = 0.0005)\n",
    "    e = Engine(model, criterion, optimizer, verbose = True, print_freq = 10)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        e.train_one_epoch(train_loader, epoch, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies from Arash's Paper\n",
    "\n",
    "Arash et al. propose that before beginning active learning, we should label 1000 random images from the target dataset and fine-tune the embedding on these first.\n",
    "\n",
    "1. What is the benefit of fine-tuning the embedding to the target dataset before beginning active learning?\n",
    "We compare 1) labeling 1000 randomly chosen images, fine-tuning the embedding, embedding these 1000 images, training a classifier, and then fine-tuning the classifier with 3000 more images chosen by uncertainty sampling, versus 2) labeling 1000 randomly chosen images, training a classifier, and then fine-tuning the classifier with 3000 more images chosen by uncertainty sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading database to get samples.\n",
      "[0, 5000, 0, 0, 0]\n",
      "Load dataset mean and std from database\n",
      "Extracting embedding from the provided model ...\n",
      "Batch 0\n",
      "Embedding extraction is done.\n"
     ]
    }
   ],
   "source": [
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "target_db = PostgresqlDatabase(DB_NAME, user=DB_USER, password=DB_PASSWORD, host='localhost')\n",
    "target_db.connect(reuse_if_open=True)\n",
    "db_proxy.initialize(target_db)\n",
    "dataset_query = Detection.select(Detection.image_id, Oracle.label, Detection.kind).join(Oracle).limit(DB_LIMIT)\n",
    "dataset = SQLDataLoader(CROP_DIR, query=dataset_query, is_training=False, kind=DetectionKind.ModelDetection.value, num_workers=8, limit=DB_LIMIT)\n",
    "dataset.updateEmbedding(model)\n",
    "sample_ids = [s[0] for s in dataset.samples]\n",
    "labels = [s[1] for s in dataset.samples]\n",
    "imagepaths = dataset.getallpaths()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_ids = np.random.choice(dataset.current_set, 1000, replace=False).tolist()\n",
    "moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, random_ids)\n",
    "\n",
    "# # Finetune the embedding model\n",
    "# dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "# dataset.train()\n",
    "# finetune_embedding(model, checkpoint['loss_type'], dataset, 20, 4, 100)\n",
    "# save_checkpoint({\n",
    "#                 'arch': checkpoint['arch'],\n",
    "#                 'state_dict': model.state_dict(),\n",
    "#                 #'optimizer' : optimizer.state_dict(),\n",
    "#                 'loss_type' : checkpoint['loss_type'],\n",
    "#                 'feat_dim' : checkpoint['feat_dim'],\n",
    "#                 'num_classes' : 21\n",
    "#                 }, False, \"%s/%s%s_%s_%04d.tar\"%(notebook_output_dirname, 'finetuned', checkpoint['loss_type'], checkpoint['arch'], 1000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed these points and train a classifier\n",
    "# dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "# dataset.updateEmbedding(model)\n",
    "dataset.embedding_mode()\n",
    "\n",
    "kwargs = {}\n",
    "kwargs[\"N\"] = 100\n",
    "kwargs[\"already_selected\"] = dataset.set_indices[DetectionKind.UserDetection.value]\n",
    "kwargs[\"model\"] = MLPClassifier(alpha=0.0001)\n",
    "\n",
    "# Train on samples that have been labeled so far\n",
    "dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "X_train = dataset.em[dataset.current_set]\n",
    "y_train = np.asarray(dataset.getlabels())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./test_sept_2019_output/original_embedding_classifier_1000.skmodel']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs[\"model\"].fit(X_train, y_train)\n",
    "joblib.dump(kwargs[\"model\"], \"%s/%s_%04d.skmodel\"%(notebook_output_dirname, 'original_embedding_classifier', 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.62575\n"
     ]
    }
   ],
   "source": [
    "accuracies_finetuned_embedding = []\n",
    "# Test on the samples that have not been labeled\n",
    "dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "dataset.embedding_mode()\n",
    "X_test = dataset.em[dataset.current_set]\n",
    "y_test = np.asarray(dataset.getlabels())\n",
    "print(\"Accuracy\", kwargs[\"model\"].score(X_test, y_test))\n",
    "accuracies_finetuned_embedding.append(kwargs[\"model\"].score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.embedding_mode()\n",
    "dataset.train()\n",
    "sampler = get_AL_sampler('confidence')(dataset.em, dataset.getalllabels(), 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 4000, 0, 1000, 0]\n",
      "Accuracy 0.6330769230769231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3900, 0, 1100, 0]\n",
      "Accuracy 0.6521052631578947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3800, 0, 1200, 0]\n",
      "Accuracy 0.6667567567567567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3700, 0, 1300, 0]\n",
      "Accuracy 0.6766666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3600, 0, 1400, 0]\n",
      "Accuracy 0.6885714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3500, 0, 1500, 0]\n",
      "Accuracy 0.6932352941176471\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3400, 0, 1600, 0]\n",
      "Accuracy 0.7233333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3300, 0, 1700, 0]\n",
      "Accuracy 0.729375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3200, 0, 1800, 0]\n",
      "Accuracy 0.7383870967741936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3100, 0, 1900, 0]\n",
      "Accuracy 0.7613333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3000, 0, 2000, 0]\n",
      "Accuracy 0.7655172413793103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2900, 0, 2100, 0]\n",
      "Accuracy 0.7882142857142858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2800, 0, 2200, 0]\n",
      "Accuracy 0.7992592592592592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2700, 0, 2300, 0]\n",
      "Accuracy 0.8026923076923077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2600, 0, 2400, 0]\n",
      "Accuracy 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2500, 0, 2500, 0]\n",
      "Accuracy 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2400, 0, 2600, 0]\n",
      "Accuracy 0.8456521739130435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2300, 0, 2700, 0]\n",
      "Accuracy 0.8622727272727273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2200, 0, 2800, 0]\n",
      "Accuracy 0.8714285714285714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2100, 0, 2900, 0]\n",
      "Accuracy 0.8835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Batch [0/300]\tTime 0.239 0.239\tLoss 1.1226 1.1226\n",
      "Epoch 0 Batch [10/300]\tTime 0.146 0.157\tLoss 1.0058 1.1679\n",
      "Epoch 0 Batch [20/300]\tTime 0.146 0.152\tLoss 0.9232 1.1625\n",
      "Epoch 0 Batch [30/300]\tTime 0.145 0.150\tLoss 1.0796 1.1361\n",
      "Epoch 0 Batch [40/300]\tTime 0.146 0.149\tLoss 0.9875 1.1176\n",
      "Epoch 0 Batch [50/300]\tTime 0.145 0.148\tLoss 1.1124 1.1058\n",
      "Epoch 0 Batch [60/300]\tTime 0.146 0.148\tLoss 1.2136 1.0926\n",
      "Epoch 0 Batch [70/300]\tTime 0.146 0.148\tLoss 1.0965 1.0899\n",
      "Epoch 0 Batch [80/300]\tTime 0.145 0.147\tLoss 1.0233 1.0897\n",
      "Epoch 0 Batch [90/300]\tTime 0.145 0.147\tLoss 1.2194 1.0972\n",
      "Epoch 0 Batch [100/300]\tTime 0.146 0.147\tLoss 1.2016 1.1000\n",
      "Epoch 0 Batch [110/300]\tTime 0.147 0.147\tLoss 1.0659 1.1081\n",
      "Epoch 0 Batch [120/300]\tTime 0.145 0.147\tLoss 1.1650 1.1093\n",
      "Epoch 0 Batch [130/300]\tTime 0.145 0.147\tLoss 1.3353 1.1093\n",
      "Epoch 0 Batch [140/300]\tTime 0.147 0.147\tLoss 1.2681 1.1086\n",
      "Epoch 0 Batch [150/300]\tTime 0.147 0.147\tLoss 1.2896 1.1089\n",
      "Epoch 0 Batch [160/300]\tTime 0.145 0.147\tLoss 1.0277 1.1085\n",
      "Epoch 0 Batch [170/300]\tTime 0.146 0.146\tLoss 1.1350 1.1067\n",
      "Epoch 0 Batch [180/300]\tTime 0.146 0.146\tLoss 1.2444 1.1110\n",
      "Epoch 0 Batch [190/300]\tTime 0.146 0.146\tLoss 1.3948 1.1172\n",
      "Epoch 0 Batch [200/300]\tTime 0.146 0.146\tLoss 0.9028 1.1159\n",
      "Epoch 0 Batch [210/300]\tTime 0.146 0.146\tLoss 0.9721 1.1145\n",
      "Epoch 0 Batch [220/300]\tTime 0.146 0.146\tLoss 1.0817 1.1140\n",
      "Epoch 0 Batch [230/300]\tTime 0.145 0.146\tLoss 1.2005 1.1153\n",
      "Epoch 0 Batch [240/300]\tTime 0.145 0.146\tLoss 1.1703 1.1132\n",
      "Epoch 0 Batch [250/300]\tTime 0.145 0.146\tLoss 0.8479 1.1115\n",
      "Epoch 0 Batch [260/300]\tTime 0.151 0.146\tLoss 1.2115 1.1093\n",
      "Epoch 0 Batch [270/300]\tTime 0.149 0.146\tLoss 1.2629 1.1050\n",
      "Epoch 0 Batch [280/300]\tTime 0.146 0.146\tLoss 1.0390 1.1019\n",
      "Epoch 0 Batch [290/300]\tTime 0.144 0.146\tLoss 0.9573 1.1003\n",
      "Epoch 1 Batch [0/300]\tTime 0.164 0.164\tLoss 0.7402 0.7402\n",
      "Epoch 1 Batch [10/300]\tTime 0.146 0.153\tLoss 1.4108 1.0391\n",
      "Epoch 1 Batch [20/300]\tTime 0.145 0.149\tLoss 0.7222 1.0516\n",
      "Epoch 1 Batch [30/300]\tTime 0.145 0.148\tLoss 1.0824 1.0565\n",
      "Epoch 1 Batch [40/300]\tTime 0.145 0.147\tLoss 0.9124 1.0528\n",
      "Epoch 1 Batch [50/300]\tTime 0.145 0.147\tLoss 0.8180 1.0310\n",
      "Epoch 1 Batch [60/300]\tTime 0.145 0.147\tLoss 0.9362 1.0214\n",
      "Epoch 1 Batch [70/300]\tTime 0.145 0.147\tLoss 1.1221 1.0245\n",
      "Epoch 1 Batch [80/300]\tTime 0.145 0.146\tLoss 0.6000 1.0243\n",
      "Epoch 1 Batch [90/300]\tTime 0.145 0.146\tLoss 1.1308 1.0201\n",
      "Epoch 1 Batch [100/300]\tTime 0.145 0.146\tLoss 0.8519 1.0225\n",
      "Epoch 1 Batch [110/300]\tTime 0.145 0.146\tLoss 1.2434 1.0286\n",
      "Epoch 1 Batch [120/300]\tTime 0.145 0.146\tLoss 1.1212 1.0351\n",
      "Epoch 1 Batch [130/300]\tTime 0.145 0.146\tLoss 1.2767 1.0432\n",
      "Epoch 1 Batch [140/300]\tTime 0.145 0.146\tLoss 1.2060 1.0448\n",
      "Epoch 1 Batch [150/300]\tTime 0.146 0.146\tLoss 0.9350 1.0519\n",
      "Epoch 1 Batch [160/300]\tTime 0.145 0.146\tLoss 1.0010 1.0540\n",
      "Epoch 1 Batch [170/300]\tTime 0.145 0.146\tLoss 0.9865 1.0518\n",
      "Epoch 1 Batch [180/300]\tTime 0.146 0.146\tLoss 0.8084 1.0428\n",
      "Epoch 1 Batch [190/300]\tTime 0.145 0.146\tLoss 1.1867 1.0424\n",
      "Epoch 1 Batch [200/300]\tTime 0.145 0.146\tLoss 1.2632 1.0457\n",
      "Epoch 1 Batch [210/300]\tTime 0.146 0.146\tLoss 1.0080 1.0393\n",
      "Epoch 1 Batch [220/300]\tTime 0.145 0.146\tLoss 1.0817 1.0379\n",
      "Epoch 1 Batch [230/300]\tTime 0.145 0.146\tLoss 1.5526 1.0409\n",
      "Epoch 1 Batch [240/300]\tTime 0.145 0.146\tLoss 1.1044 1.0420\n",
      "Epoch 1 Batch [250/300]\tTime 0.145 0.146\tLoss 1.2392 1.0445\n",
      "Epoch 1 Batch [260/300]\tTime 0.145 0.146\tLoss 1.0028 1.0408\n",
      "Epoch 1 Batch [270/300]\tTime 0.145 0.146\tLoss 1.0531 1.0438\n",
      "Epoch 1 Batch [280/300]\tTime 0.145 0.146\tLoss 0.4738 1.0433\n",
      "Epoch 1 Batch [290/300]\tTime 0.145 0.146\tLoss 0.9270 1.0410\n",
      "Epoch 2 Batch [0/300]\tTime 0.164 0.164\tLoss 0.9549 0.9549\n",
      "Epoch 2 Batch [10/300]\tTime 0.145 0.152\tLoss 0.6989 0.9186\n",
      "Epoch 2 Batch [20/300]\tTime 0.145 0.149\tLoss 0.9610 0.9317\n",
      "Epoch 2 Batch [30/300]\tTime 0.145 0.148\tLoss 1.0264 0.9676\n",
      "Epoch 2 Batch [40/300]\tTime 0.145 0.147\tLoss 1.1043 0.9719\n",
      "Epoch 2 Batch [50/300]\tTime 0.145 0.147\tLoss 0.8376 0.9836\n",
      "Epoch 2 Batch [60/300]\tTime 0.145 0.146\tLoss 1.1524 0.9799\n",
      "Epoch 2 Batch [70/300]\tTime 0.145 0.146\tLoss 0.9816 0.9857\n",
      "Epoch 2 Batch [80/300]\tTime 0.145 0.146\tLoss 1.3174 0.9855\n",
      "Epoch 2 Batch [90/300]\tTime 0.147 0.146\tLoss 1.3665 1.0095\n",
      "Epoch 2 Batch [100/300]\tTime 0.146 0.146\tLoss 0.8721 1.0075\n",
      "Epoch 2 Batch [110/300]\tTime 0.145 0.146\tLoss 0.7349 1.0011\n",
      "Epoch 2 Batch [120/300]\tTime 0.145 0.146\tLoss 1.3314 1.0056\n",
      "Epoch 2 Batch [130/300]\tTime 0.145 0.146\tLoss 1.2680 1.0064\n",
      "Epoch 2 Batch [140/300]\tTime 0.145 0.146\tLoss 1.4089 1.0092\n",
      "Epoch 2 Batch [150/300]\tTime 0.145 0.146\tLoss 0.8687 1.0088\n",
      "Epoch 2 Batch [160/300]\tTime 0.146 0.146\tLoss 0.8232 1.0045\n",
      "Epoch 2 Batch [170/300]\tTime 0.145 0.146\tLoss 1.2196 1.0063\n",
      "Epoch 2 Batch [180/300]\tTime 0.145 0.146\tLoss 1.5689 1.0124\n",
      "Epoch 2 Batch [190/300]\tTime 0.145 0.146\tLoss 1.0037 1.0154\n",
      "Epoch 2 Batch [200/300]\tTime 0.145 0.146\tLoss 1.2755 1.0239\n",
      "Epoch 2 Batch [210/300]\tTime 0.145 0.146\tLoss 0.9904 1.0180\n",
      "Epoch 2 Batch [220/300]\tTime 0.144 0.146\tLoss 0.2652 1.0183\n",
      "Epoch 2 Batch [230/300]\tTime 0.145 0.146\tLoss 1.2698 1.0145\n",
      "Epoch 2 Batch [240/300]\tTime 0.145 0.146\tLoss 1.1534 1.0126\n",
      "Epoch 2 Batch [250/300]\tTime 0.145 0.145\tLoss 1.3069 1.0121\n",
      "Epoch 2 Batch [260/300]\tTime 0.145 0.145\tLoss 1.1518 1.0235\n",
      "Epoch 2 Batch [270/300]\tTime 0.144 0.145\tLoss 0.2603 1.0259\n",
      "Epoch 2 Batch [280/300]\tTime 0.147 0.145\tLoss 0.6360 1.0180\n",
      "Epoch 2 Batch [290/300]\tTime 0.145 0.145\tLoss 0.8925 1.0192\n",
      "Epoch 3 Batch [0/300]\tTime 0.153 0.153\tLoss 1.2047 1.2047\n",
      "Epoch 3 Batch [10/300]\tTime 0.145 0.156\tLoss 1.2105 0.9475\n",
      "Epoch 3 Batch [20/300]\tTime 0.145 0.151\tLoss 1.1001 1.0247\n",
      "Epoch 3 Batch [30/300]\tTime 0.145 0.149\tLoss 1.4784 1.0452\n",
      "Epoch 3 Batch [40/300]\tTime 0.144 0.148\tLoss 1.1841 1.0765\n",
      "Epoch 3 Batch [50/300]\tTime 0.145 0.147\tLoss 1.4207 1.0933\n",
      "Epoch 3 Batch [60/300]\tTime 0.146 0.147\tLoss 1.0363 1.1049\n",
      "Epoch 3 Batch [70/300]\tTime 0.145 0.147\tLoss 0.7798 1.1542\n",
      "Epoch 3 Batch [80/300]\tTime 0.146 0.147\tLoss 1.0433 1.1480\n",
      "Epoch 3 Batch [90/300]\tTime 0.145 0.146\tLoss 0.4083 1.1160\n",
      "Epoch 3 Batch [100/300]\tTime 0.148 0.146\tLoss 0.9821 1.0950\n",
      "Epoch 3 Batch [110/300]\tTime 0.145 0.146\tLoss 1.2670 1.1019\n",
      "Epoch 3 Batch [120/300]\tTime 0.145 0.146\tLoss 1.0312 1.1072\n",
      "Epoch 3 Batch [130/300]\tTime 0.145 0.146\tLoss 1.1166 1.1002\n",
      "Epoch 3 Batch [140/300]\tTime 0.145 0.146\tLoss 1.1393 1.0967\n",
      "Epoch 3 Batch [150/300]\tTime 0.146 0.146\tLoss 1.0969 1.0946\n",
      "Epoch 3 Batch [160/300]\tTime 0.145 0.146\tLoss 0.9222 1.0902\n",
      "Epoch 3 Batch [170/300]\tTime 0.145 0.146\tLoss 1.0653 1.0910\n",
      "Epoch 3 Batch [180/300]\tTime 0.145 0.146\tLoss 0.5250 1.0785\n",
      "Epoch 3 Batch [190/300]\tTime 0.146 0.146\tLoss 1.1931 1.0843\n",
      "Epoch 3 Batch [200/300]\tTime 0.144 0.146\tLoss 1.2063 1.0662\n",
      "Epoch 3 Batch [210/300]\tTime 0.145 0.146\tLoss 1.2522 1.0656\n",
      "Epoch 3 Batch [220/300]\tTime 0.145 0.146\tLoss 0.8686 1.0643\n",
      "Epoch 3 Batch [230/300]\tTime 0.145 0.146\tLoss 1.0397 1.0678\n",
      "Epoch 3 Batch [240/300]\tTime 0.145 0.146\tLoss 1.6493 1.0644\n",
      "Epoch 3 Batch [250/300]\tTime 0.145 0.146\tLoss 1.3881 1.0596\n",
      "Epoch 3 Batch [260/300]\tTime 0.145 0.146\tLoss 1.4295 1.0644\n",
      "Epoch 3 Batch [270/300]\tTime 0.145 0.146\tLoss 0.9200 1.0560\n",
      "Epoch 3 Batch [280/300]\tTime 0.145 0.145\tLoss 0.8759 1.0514\n",
      "Epoch 3 Batch [290/300]\tTime 0.144 0.145\tLoss 1.3423 1.0485\n",
      "Epoch 4 Batch [0/300]\tTime 0.168 0.168\tLoss 1.0636 1.0636\n",
      "Epoch 4 Batch [10/300]\tTime 0.145 0.153\tLoss 1.7544 1.0285\n",
      "Epoch 4 Batch [20/300]\tTime 0.145 0.149\tLoss 0.7601 1.0138\n",
      "Epoch 4 Batch [30/300]\tTime 0.145 0.148\tLoss 1.2910 1.0196\n",
      "Epoch 4 Batch [40/300]\tTime 0.146 0.147\tLoss 0.8467 1.0350\n",
      "Epoch 4 Batch [50/300]\tTime 0.145 0.147\tLoss 1.3974 1.0289\n",
      "Epoch 4 Batch [60/300]\tTime 0.145 0.146\tLoss 1.1582 1.0459\n",
      "Epoch 4 Batch [70/300]\tTime 0.147 0.146\tLoss 0.5303 1.0139\n",
      "Epoch 4 Batch [80/300]\tTime 0.147 0.146\tLoss 1.8186 1.0469\n",
      "Epoch 4 Batch [90/300]\tTime 0.145 0.146\tLoss 1.2291 1.0530\n",
      "Epoch 4 Batch [100/300]\tTime 0.144 0.146\tLoss 0.3778 1.0435\n",
      "Epoch 4 Batch [110/300]\tTime 0.145 0.146\tLoss 1.2006 1.0304\n",
      "Epoch 4 Batch [120/300]\tTime 0.145 0.146\tLoss 1.1401 1.0144\n",
      "Epoch 4 Batch [130/300]\tTime 0.145 0.146\tLoss 0.4131 1.0107\n",
      "Epoch 4 Batch [140/300]\tTime 0.144 0.145\tLoss 0.2728 1.0074\n",
      "Epoch 4 Batch [150/300]\tTime 0.145 0.145\tLoss 1.3258 1.0333\n",
      "Epoch 4 Batch [160/300]\tTime 0.145 0.145\tLoss 0.4976 1.0322\n",
      "Epoch 4 Batch [170/300]\tTime 0.145 0.145\tLoss 1.0826 1.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch [180/300]\tTime 0.144 0.145\tLoss 0.4628 1.0342\n",
      "Epoch 4 Batch [190/300]\tTime 0.145 0.145\tLoss 1.3208 1.0344\n",
      "Epoch 4 Batch [200/300]\tTime 0.145 0.145\tLoss 0.3877 1.0327\n",
      "Epoch 4 Batch [210/300]\tTime 0.144 0.145\tLoss 1.1162 1.0237\n",
      "Epoch 4 Batch [220/300]\tTime 0.145 0.145\tLoss 0.8010 1.0296\n",
      "Epoch 4 Batch [230/300]\tTime 0.145 0.145\tLoss 1.3302 1.0264\n",
      "Epoch 4 Batch [240/300]\tTime 0.145 0.145\tLoss 0.9688 1.0200\n",
      "Epoch 4 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 1.0138\n",
      "Epoch 4 Batch [260/300]\tTime 0.145 0.145\tLoss 0.2938 1.0001\n",
      "Epoch 4 Batch [270/300]\tTime 0.145 0.145\tLoss 0.5555 0.9989\n",
      "Epoch 4 Batch [280/300]\tTime 0.145 0.145\tLoss 0.9163 1.0065\n",
      "Epoch 4 Batch [290/300]\tTime 0.144 0.145\tLoss 1.1945 0.9973\n",
      "Epoch 5 Batch [0/300]\tTime 0.174 0.174\tLoss 1.3808 1.3808\n",
      "Epoch 5 Batch [10/300]\tTime 0.145 0.158\tLoss 0.7775 1.2285\n",
      "Epoch 5 Batch [20/300]\tTime 0.145 0.152\tLoss 1.5328 1.1639\n",
      "Epoch 5 Batch [30/300]\tTime 0.145 0.149\tLoss 1.1439 1.1591\n",
      "Epoch 5 Batch [40/300]\tTime 0.145 0.148\tLoss 1.0673 1.1425\n",
      "Epoch 5 Batch [50/300]\tTime 0.144 0.148\tLoss 0.4989 1.1144\n",
      "Epoch 5 Batch [60/300]\tTime 0.144 0.147\tLoss 1.3693 1.1253\n",
      "Epoch 5 Batch [70/300]\tTime 0.145 0.147\tLoss 1.3080 1.1123\n",
      "Epoch 5 Batch [80/300]\tTime 0.145 0.147\tLoss 1.2791 1.1363\n",
      "Epoch 5 Batch [90/300]\tTime 0.145 0.146\tLoss 0.8618 1.1261\n",
      "Epoch 5 Batch [100/300]\tTime 0.145 0.146\tLoss 1.0638 1.0936\n",
      "Epoch 5 Batch [110/300]\tTime 0.144 0.146\tLoss 1.1898 1.1114\n",
      "Epoch 5 Batch [120/300]\tTime 0.146 0.146\tLoss 0.4776 1.1006\n",
      "Epoch 5 Batch [130/300]\tTime 0.145 0.146\tLoss 2.2015 1.0992\n",
      "Epoch 5 Batch [140/300]\tTime 0.145 0.146\tLoss 0.0000 1.0626\n",
      "Epoch 5 Batch [150/300]\tTime 0.145 0.146\tLoss 0.9404 1.0589\n",
      "Epoch 5 Batch [160/300]\tTime 0.144 0.146\tLoss 1.2243 1.0619\n",
      "Epoch 5 Batch [170/300]\tTime 0.144 0.146\tLoss 1.1269 1.0666\n",
      "Epoch 5 Batch [180/300]\tTime 0.145 0.146\tLoss 0.0000 1.0723\n",
      "Epoch 5 Batch [190/300]\tTime 0.144 0.146\tLoss 0.9628 1.0755\n",
      "Epoch 5 Batch [200/300]\tTime 0.144 0.146\tLoss 0.0000 1.0491\n",
      "Epoch 5 Batch [210/300]\tTime 0.144 0.145\tLoss 1.9375 1.0453\n",
      "Epoch 5 Batch [220/300]\tTime 0.144 0.145\tLoss 0.7294 1.0441\n",
      "Epoch 5 Batch [230/300]\tTime 0.145 0.145\tLoss 1.2656 1.0393\n",
      "Epoch 5 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0108 1.0402\n",
      "Epoch 5 Batch [250/300]\tTime 0.144 0.145\tLoss 0.4461 1.0347\n",
      "Epoch 5 Batch [260/300]\tTime 0.145 0.145\tLoss 0.8723 1.0169\n",
      "Epoch 5 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 1.0102\n",
      "Epoch 5 Batch [280/300]\tTime 0.145 0.145\tLoss 0.7340 1.0088\n",
      "Epoch 5 Batch [290/300]\tTime 0.144 0.145\tLoss 0.9741 1.0019\n",
      "Epoch 6 Batch [0/300]\tTime 0.168 0.168\tLoss 0.0000 0.0000\n",
      "Epoch 6 Batch [10/300]\tTime 0.145 0.147\tLoss 0.8935 1.0757\n",
      "Epoch 6 Batch [20/300]\tTime 0.144 0.146\tLoss 0.9961 0.9551\n",
      "Epoch 6 Batch [30/300]\tTime 0.145 0.146\tLoss 0.2389 1.0066\n",
      "Epoch 6 Batch [40/300]\tTime 0.145 0.146\tLoss 1.4166 0.9785\n",
      "Epoch 6 Batch [50/300]\tTime 0.144 0.145\tLoss 0.0000 0.9325\n",
      "Epoch 6 Batch [60/300]\tTime 0.145 0.145\tLoss 1.2616 0.9623\n",
      "Epoch 6 Batch [70/300]\tTime 0.145 0.145\tLoss 1.2413 0.9344\n",
      "Epoch 6 Batch [80/300]\tTime 0.144 0.145\tLoss 1.0444 0.9486\n",
      "Epoch 6 Batch [90/300]\tTime 0.145 0.145\tLoss 0.8269 0.9484\n",
      "Epoch 6 Batch [100/300]\tTime 0.145 0.145\tLoss 0.0000 0.9211\n",
      "Epoch 6 Batch [110/300]\tTime 0.144 0.145\tLoss 0.5642 0.8860\n",
      "Epoch 6 Batch [120/300]\tTime 0.144 0.145\tLoss 3.4876 0.8658\n",
      "Epoch 6 Batch [130/300]\tTime 0.144 0.145\tLoss 0.4477 0.8565\n",
      "Epoch 6 Batch [140/300]\tTime 0.145 0.145\tLoss 2.0933 0.8542\n",
      "Epoch 6 Batch [150/300]\tTime 0.144 0.145\tLoss 0.7595 0.8487\n",
      "Epoch 6 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.8728\n",
      "Epoch 6 Batch [170/300]\tTime 0.144 0.145\tLoss 1.7934 0.9011\n",
      "Epoch 6 Batch [180/300]\tTime 0.145 0.145\tLoss 1.4306 0.8954\n",
      "Epoch 6 Batch [190/300]\tTime 0.144 0.145\tLoss 0.5157 0.9003\n",
      "Epoch 6 Batch [200/300]\tTime 0.145 0.145\tLoss 0.6287 0.9013\n",
      "Epoch 6 Batch [210/300]\tTime 0.144 0.145\tLoss 0.5623 0.8837\n",
      "Epoch 6 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.8753\n",
      "Epoch 6 Batch [230/300]\tTime 0.144 0.145\tLoss 0.1517 0.8572\n",
      "Epoch 6 Batch [240/300]\tTime 0.144 0.145\tLoss 0.7438 0.8431\n",
      "Epoch 6 Batch [250/300]\tTime 0.145 0.145\tLoss 1.5078 0.8551\n",
      "Epoch 6 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.8511\n",
      "Epoch 6 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.8385\n",
      "Epoch 6 Batch [280/300]\tTime 0.145 0.145\tLoss 1.6927 0.8432\n",
      "Epoch 6 Batch [290/300]\tTime 0.144 0.145\tLoss 2.0837 0.8450\n",
      "Epoch 7 Batch [0/300]\tTime 0.153 0.153\tLoss 0.0000 0.0000\n",
      "Epoch 7 Batch [10/300]\tTime 0.145 0.157\tLoss 1.3479 0.7250\n",
      "Epoch 7 Batch [20/300]\tTime 0.144 0.151\tLoss 0.1364 0.7201\n",
      "Epoch 7 Batch [30/300]\tTime 0.145 0.149\tLoss 0.0000 0.7304\n",
      "Epoch 7 Batch [40/300]\tTime 0.145 0.148\tLoss 1.1199 0.7194\n",
      "Epoch 7 Batch [50/300]\tTime 0.144 0.147\tLoss 1.7341 0.6847\n",
      "Epoch 7 Batch [60/300]\tTime 0.149 0.147\tLoss 1.7442 0.7204\n",
      "Epoch 7 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.6755\n",
      "Epoch 7 Batch [80/300]\tTime 0.145 0.146\tLoss 1.0372 0.6770\n",
      "Epoch 7 Batch [90/300]\tTime 0.145 0.146\tLoss 0.3864 0.6613\n",
      "Epoch 7 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.6958\n",
      "Epoch 7 Batch [110/300]\tTime 0.144 0.146\tLoss 1.1662 0.7201\n",
      "Epoch 7 Batch [120/300]\tTime 0.151 0.146\tLoss 0.1907 0.7405\n",
      "Epoch 7 Batch [130/300]\tTime 0.145 0.146\tLoss 0.0000 0.7385\n",
      "Epoch 7 Batch [140/300]\tTime 0.145 0.146\tLoss 0.0000 0.7344\n",
      "Epoch 7 Batch [150/300]\tTime 0.145 0.146\tLoss 0.9761 0.7812\n",
      "Epoch 7 Batch [160/300]\tTime 0.145 0.146\tLoss 1.0467 0.7837\n",
      "Epoch 7 Batch [170/300]\tTime 0.150 0.145\tLoss 3.7398 0.7827\n",
      "Epoch 7 Batch [180/300]\tTime 0.145 0.145\tLoss 1.5913 0.7924\n",
      "Epoch 7 Batch [190/300]\tTime 0.144 0.145\tLoss 1.3585 0.8059\n",
      "Epoch 7 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.7921\n",
      "Epoch 7 Batch [210/300]\tTime 0.145 0.145\tLoss 1.8258 0.8058\n",
      "Epoch 7 Batch [220/300]\tTime 0.145 0.145\tLoss 1.5544 0.8189\n",
      "Epoch 7 Batch [230/300]\tTime 0.149 0.145\tLoss 0.7337 0.8224\n",
      "Epoch 7 Batch [240/300]\tTime 0.145 0.145\tLoss 0.7576 0.8251\n",
      "Epoch 7 Batch [250/300]\tTime 0.145 0.145\tLoss 0.1452 0.8105\n",
      "Epoch 7 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.8047\n",
      "Epoch 7 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.7854\n",
      "Epoch 7 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.7883\n",
      "Epoch 7 Batch [290/300]\tTime 0.144 0.145\tLoss 1.6474 0.7941\n",
      "Epoch 8 Batch [0/300]\tTime 0.166 0.166\tLoss 0.0000 0.0000\n",
      "Epoch 8 Batch [10/300]\tTime 0.145 0.148\tLoss 0.0000 0.5437\n",
      "Epoch 8 Batch [20/300]\tTime 0.145 0.146\tLoss 1.3116 0.5876\n",
      "Epoch 8 Batch [30/300]\tTime 0.144 0.146\tLoss 0.0000 0.5799\n",
      "Epoch 8 Batch [40/300]\tTime 0.145 0.145\tLoss 1.6777 0.6319\n",
      "Epoch 8 Batch [50/300]\tTime 0.145 0.145\tLoss 2.0164 0.6318\n",
      "Epoch 8 Batch [60/300]\tTime 0.146 0.145\tLoss 0.0952 0.6359\n",
      "Epoch 8 Batch [70/300]\tTime 0.145 0.145\tLoss 0.2164 0.6194\n",
      "Epoch 8 Batch [80/300]\tTime 0.146 0.145\tLoss 0.0000 0.6401\n",
      "Epoch 8 Batch [90/300]\tTime 0.144 0.145\tLoss 1.6930 0.6828\n",
      "Epoch 8 Batch [100/300]\tTime 0.144 0.145\tLoss 0.6393 0.7161\n",
      "Epoch 8 Batch [110/300]\tTime 0.145 0.145\tLoss 0.6851 0.6890\n",
      "Epoch 8 Batch [120/300]\tTime 0.145 0.145\tLoss 0.9951 0.7061\n",
      "Epoch 8 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.7200\n",
      "Epoch 8 Batch [140/300]\tTime 0.145 0.145\tLoss 0.5461 0.7181\n",
      "Epoch 8 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.7231\n",
      "Epoch 8 Batch [160/300]\tTime 0.145 0.145\tLoss 0.2929 0.7070\n",
      "Epoch 8 Batch [170/300]\tTime 0.146 0.145\tLoss 0.0000 0.6773\n",
      "Epoch 8 Batch [180/300]\tTime 0.145 0.145\tLoss 0.3518 0.6776\n",
      "Epoch 8 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.6710\n",
      "Epoch 8 Batch [200/300]\tTime 0.145 0.145\tLoss 1.4378 0.6732\n",
      "Epoch 8 Batch [210/300]\tTime 0.145 0.145\tLoss 1.3700 0.6786\n",
      "Epoch 8 Batch [220/300]\tTime 0.144 0.145\tLoss 0.5616 0.6636\n",
      "Epoch 8 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.6606\n",
      "Epoch 8 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.6574\n",
      "Epoch 8 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.6504\n",
      "Epoch 8 Batch [260/300]\tTime 0.145 0.145\tLoss 0.4859 0.6655\n",
      "Epoch 8 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.6652\n",
      "Epoch 8 Batch [280/300]\tTime 0.145 0.145\tLoss 3.3041 0.6653\n",
      "Epoch 8 Batch [290/300]\tTime 0.144 0.145\tLoss 0.2848 0.6559\n",
      "Epoch 9 Batch [0/300]\tTime 0.170 0.170\tLoss 0.3165 0.3165\n",
      "Epoch 9 Batch [10/300]\tTime 0.144 0.147\tLoss 0.0478 0.7185\n",
      "Epoch 9 Batch [20/300]\tTime 0.144 0.146\tLoss 0.0000 0.7378\n",
      "Epoch 9 Batch [30/300]\tTime 0.146 0.145\tLoss 1.2355 0.6378\n",
      "Epoch 9 Batch [40/300]\tTime 0.144 0.145\tLoss 0.0000 0.5732\n",
      "Epoch 9 Batch [50/300]\tTime 0.144 0.145\tLoss 0.8419 0.5580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Batch [60/300]\tTime 0.145 0.145\tLoss 1.0493 0.5614\n",
      "Epoch 9 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.5629\n",
      "Epoch 9 Batch [80/300]\tTime 0.144 0.145\tLoss 1.5825 0.5655\n",
      "Epoch 9 Batch [90/300]\tTime 0.145 0.145\tLoss 0.7230 0.6061\n",
      "Epoch 9 Batch [100/300]\tTime 0.145 0.145\tLoss 3.1355 0.6465\n",
      "Epoch 9 Batch [110/300]\tTime 0.144 0.145\tLoss 0.8445 0.6171\n",
      "Epoch 9 Batch [120/300]\tTime 0.144 0.145\tLoss 0.4441 0.6479\n",
      "Epoch 9 Batch [130/300]\tTime 0.145 0.145\tLoss 0.9046 0.6333\n",
      "Epoch 9 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.6430\n",
      "Epoch 9 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.6359\n",
      "Epoch 9 Batch [160/300]\tTime 0.145 0.145\tLoss 1.0523 0.6418\n",
      "Epoch 9 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.6387\n",
      "Epoch 9 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.6313\n",
      "Epoch 9 Batch [190/300]\tTime 0.148 0.145\tLoss 0.8412 0.6451\n",
      "Epoch 9 Batch [200/300]\tTime 0.146 0.145\tLoss 1.5095 0.6522\n",
      "Epoch 9 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.6542\n",
      "Epoch 9 Batch [220/300]\tTime 0.145 0.145\tLoss 0.8150 0.6532\n",
      "Epoch 9 Batch [230/300]\tTime 0.145 0.145\tLoss 0.2558 0.6390\n",
      "Epoch 9 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.6151\n",
      "Epoch 9 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.6168\n",
      "Epoch 9 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.6115\n",
      "Epoch 9 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.5915\n",
      "Epoch 9 Batch [280/300]\tTime 0.145 0.145\tLoss 1.6824 0.5895\n",
      "Epoch 9 Batch [290/300]\tTime 0.144 0.145\tLoss 0.3619 0.5704\n",
      "Epoch 10 Batch [0/300]\tTime 0.150 0.150\tLoss 0.2915 0.2915\n",
      "Epoch 10 Batch [10/300]\tTime 0.150 0.153\tLoss 0.0000 0.4844\n",
      "Epoch 10 Batch [20/300]\tTime 0.144 0.150\tLoss 0.8396 0.3735\n",
      "Epoch 10 Batch [30/300]\tTime 0.144 0.148\tLoss 0.6496 0.5037\n",
      "Epoch 10 Batch [40/300]\tTime 0.145 0.147\tLoss 0.0000 0.4714\n",
      "Epoch 10 Batch [50/300]\tTime 0.146 0.147\tLoss 1.4843 0.4843\n",
      "Epoch 10 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.5005\n",
      "Epoch 10 Batch [70/300]\tTime 0.147 0.146\tLoss 0.5406 0.4663\n",
      "Epoch 10 Batch [80/300]\tTime 0.145 0.146\tLoss 0.0000 0.4750\n",
      "Epoch 10 Batch [90/300]\tTime 0.144 0.146\tLoss 0.7315 0.4774\n",
      "Epoch 10 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.4971\n",
      "Epoch 10 Batch [110/300]\tTime 0.144 0.146\tLoss 0.0000 0.4957\n",
      "Epoch 10 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0000 0.5206\n",
      "Epoch 10 Batch [130/300]\tTime 0.145 0.146\tLoss 0.0000 0.4992\n",
      "Epoch 10 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.4874\n",
      "Epoch 10 Batch [150/300]\tTime 0.147 0.145\tLoss 0.0000 0.4876\n",
      "Epoch 10 Batch [160/300]\tTime 0.144 0.145\tLoss 0.2090 0.4837\n",
      "Epoch 10 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.4803\n",
      "Epoch 10 Batch [180/300]\tTime 0.149 0.145\tLoss 0.6021 0.4660\n",
      "Epoch 10 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.4686\n",
      "Epoch 10 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.4561\n",
      "Epoch 10 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.4685\n",
      "Epoch 10 Batch [220/300]\tTime 0.145 0.145\tLoss 0.3852 0.4745\n",
      "Epoch 10 Batch [230/300]\tTime 0.144 0.145\tLoss 0.6689 0.4864\n",
      "Epoch 10 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.4816\n",
      "Epoch 10 Batch [250/300]\tTime 0.145 0.145\tLoss 2.2302 0.4804\n",
      "Epoch 10 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.4814\n",
      "Epoch 10 Batch [270/300]\tTime 0.144 0.145\tLoss 0.4629 0.4889\n",
      "Epoch 10 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.4796\n",
      "Epoch 10 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.4741\n",
      "Epoch 11 Batch [0/300]\tTime 0.153 0.153\tLoss 0.5411 0.5411\n",
      "Epoch 11 Batch [10/300]\tTime 0.144 0.153\tLoss 0.0000 0.7887\n",
      "Epoch 11 Batch [20/300]\tTime 0.144 0.149\tLoss 0.0000 0.5937\n",
      "Epoch 11 Batch [30/300]\tTime 0.145 0.148\tLoss 0.2232 0.6111\n",
      "Epoch 11 Batch [40/300]\tTime 0.145 0.147\tLoss 2.7317 0.6977\n",
      "Epoch 11 Batch [50/300]\tTime 0.144 0.147\tLoss 0.0000 0.6620\n",
      "Epoch 11 Batch [60/300]\tTime 0.144 0.146\tLoss 2.8334 0.6549\n",
      "Epoch 11 Batch [70/300]\tTime 0.145 0.146\tLoss 1.0920 0.6853\n",
      "Epoch 11 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.6829\n",
      "Epoch 11 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.6266\n",
      "Epoch 11 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.5801\n",
      "Epoch 11 Batch [110/300]\tTime 0.144 0.146\tLoss 0.0000 0.5339\n",
      "Epoch 11 Batch [120/300]\tTime 0.145 0.146\tLoss 0.6748 0.5428\n",
      "Epoch 11 Batch [130/300]\tTime 0.145 0.145\tLoss 0.4805 0.5193\n",
      "Epoch 11 Batch [140/300]\tTime 0.144 0.145\tLoss 0.5275 0.5185\n",
      "Epoch 11 Batch [150/300]\tTime 0.145 0.145\tLoss 0.5489 0.5276\n",
      "Epoch 11 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.5234\n",
      "Epoch 11 Batch [170/300]\tTime 0.145 0.145\tLoss 2.1096 0.5427\n",
      "Epoch 11 Batch [180/300]\tTime 0.149 0.145\tLoss 1.8398 0.5592\n",
      "Epoch 11 Batch [190/300]\tTime 0.145 0.145\tLoss 1.3683 0.5522\n",
      "Epoch 11 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.5490\n",
      "Epoch 11 Batch [210/300]\tTime 0.146 0.145\tLoss 0.0000 0.5279\n",
      "Epoch 11 Batch [220/300]\tTime 0.145 0.145\tLoss 0.7798 0.5330\n",
      "Epoch 11 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.5311\n",
      "Epoch 11 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.5259\n",
      "Epoch 11 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.5215\n",
      "Epoch 11 Batch [260/300]\tTime 0.145 0.145\tLoss 0.2282 0.5169\n",
      "Epoch 11 Batch [270/300]\tTime 0.145 0.145\tLoss 0.9255 0.5043\n",
      "Epoch 11 Batch [280/300]\tTime 0.145 0.145\tLoss 2.2600 0.5135\n",
      "Epoch 11 Batch [290/300]\tTime 0.144 0.145\tLoss 1.4933 0.5083\n",
      "Epoch 12 Batch [0/300]\tTime 0.164 0.164\tLoss 0.0000 0.0000\n",
      "Epoch 12 Batch [10/300]\tTime 0.144 0.146\tLoss 0.0000 0.1268\n",
      "Epoch 12 Batch [20/300]\tTime 0.147 0.146\tLoss 0.0000 0.3393\n",
      "Epoch 12 Batch [30/300]\tTime 0.144 0.145\tLoss 0.0000 0.3077\n",
      "Epoch 12 Batch [40/300]\tTime 0.144 0.145\tLoss 0.0000 0.3221\n",
      "Epoch 12 Batch [50/300]\tTime 0.144 0.145\tLoss 0.0000 0.3521\n",
      "Epoch 12 Batch [60/300]\tTime 0.144 0.145\tLoss 0.3263 0.3243\n",
      "Epoch 12 Batch [70/300]\tTime 0.145 0.145\tLoss 0.0000 0.2849\n",
      "Epoch 12 Batch [80/300]\tTime 0.145 0.145\tLoss 0.0000 0.4037\n",
      "Epoch 12 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.3594\n",
      "Epoch 12 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.3738\n",
      "Epoch 12 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.3778\n",
      "Epoch 12 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.3776\n",
      "Epoch 12 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.3803\n",
      "Epoch 12 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.3982\n",
      "Epoch 12 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.3957\n",
      "Epoch 12 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.3723\n",
      "Epoch 12 Batch [170/300]\tTime 0.144 0.145\tLoss 0.8650 0.3757\n",
      "Epoch 12 Batch [180/300]\tTime 0.145 0.145\tLoss 0.3583 0.3842\n",
      "Epoch 12 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.4023\n",
      "Epoch 12 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.3867\n",
      "Epoch 12 Batch [210/300]\tTime 0.145 0.145\tLoss 0.0000 0.3756\n",
      "Epoch 12 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.3820\n",
      "Epoch 12 Batch [230/300]\tTime 0.144 0.145\tLoss 0.9737 0.3911\n",
      "Epoch 12 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.3857\n",
      "Epoch 12 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.4057\n",
      "Epoch 12 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.4154\n",
      "Epoch 12 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.4049\n",
      "Epoch 12 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.3992\n",
      "Epoch 12 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.4230\n",
      "Epoch 13 Batch [0/300]\tTime 0.153 0.153\tLoss 0.0000 0.0000\n",
      "Epoch 13 Batch [10/300]\tTime 0.145 0.154\tLoss 0.6652 0.1133\n",
      "Epoch 13 Batch [20/300]\tTime 0.144 0.149\tLoss 0.0000 0.3739\n",
      "Epoch 13 Batch [30/300]\tTime 0.145 0.148\tLoss 0.4722 0.3376\n",
      "Epoch 13 Batch [40/300]\tTime 0.144 0.147\tLoss 0.0000 0.2678\n",
      "Epoch 13 Batch [50/300]\tTime 0.145 0.147\tLoss 3.4117 0.3303\n",
      "Epoch 13 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.3134\n",
      "Epoch 13 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0864 0.3028\n",
      "Epoch 13 Batch [80/300]\tTime 0.145 0.146\tLoss 0.4573 0.3001\n",
      "Epoch 13 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.2751\n",
      "Epoch 13 Batch [100/300]\tTime 0.144 0.146\tLoss 1.4837 0.2720\n",
      "Epoch 13 Batch [110/300]\tTime 0.145 0.146\tLoss 0.0000 0.2750\n",
      "Epoch 13 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0132 0.2912\n",
      "Epoch 13 Batch [130/300]\tTime 0.146 0.145\tLoss 3.4400 0.3045\n",
      "Epoch 13 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2829\n",
      "Epoch 13 Batch [150/300]\tTime 0.144 0.145\tLoss 0.5870 0.2859\n",
      "Epoch 13 Batch [160/300]\tTime 0.144 0.145\tLoss 2.5287 0.2925\n",
      "Epoch 13 Batch [170/300]\tTime 0.145 0.145\tLoss 0.0000 0.3038\n",
      "Epoch 13 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.2951\n",
      "Epoch 13 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.2849\n",
      "Epoch 13 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0631 0.2849\n",
      "Epoch 13 Batch [210/300]\tTime 0.149 0.145\tLoss 0.9533 0.3044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.2906\n",
      "Epoch 13 Batch [230/300]\tTime 0.145 0.145\tLoss 0.9156 0.2919\n",
      "Epoch 13 Batch [240/300]\tTime 0.146 0.145\tLoss 0.0000 0.2970\n",
      "Epoch 13 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.3019\n",
      "Epoch 13 Batch [260/300]\tTime 0.144 0.145\tLoss 0.4763 0.3131\n",
      "Epoch 13 Batch [270/300]\tTime 0.144 0.145\tLoss 1.2463 0.3204\n",
      "Epoch 13 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.3175\n",
      "Epoch 13 Batch [290/300]\tTime 0.143 0.145\tLoss 1.6160 0.3252\n",
      "Epoch 14 Batch [0/300]\tTime 0.157 0.157\tLoss 0.6480 0.6480\n",
      "Epoch 14 Batch [10/300]\tTime 0.144 0.149\tLoss 0.0000 0.4294\n",
      "Epoch 14 Batch [20/300]\tTime 0.144 0.147\tLoss 0.0000 0.4268\n",
      "Epoch 14 Batch [30/300]\tTime 0.144 0.146\tLoss 0.0000 0.5034\n",
      "Epoch 14 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.5218\n",
      "Epoch 14 Batch [50/300]\tTime 0.147 0.146\tLoss 0.2270 0.5643\n",
      "Epoch 14 Batch [60/300]\tTime 0.144 0.145\tLoss 0.0000 0.6067\n",
      "Epoch 14 Batch [70/300]\tTime 0.145 0.145\tLoss 0.2892 0.5405\n",
      "Epoch 14 Batch [80/300]\tTime 0.145 0.145\tLoss 1.4164 0.5002\n",
      "Epoch 14 Batch [90/300]\tTime 0.145 0.145\tLoss 1.1769 0.4943\n",
      "Epoch 14 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.4796\n",
      "Epoch 14 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.4484\n",
      "Epoch 14 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.4923\n",
      "Epoch 14 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.4846\n",
      "Epoch 14 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.5310\n",
      "Epoch 14 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.5145\n",
      "Epoch 14 Batch [160/300]\tTime 0.144 0.145\tLoss 0.4330 0.5106\n",
      "Epoch 14 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.5083\n",
      "Epoch 14 Batch [180/300]\tTime 0.144 0.145\tLoss 0.7283 0.5005\n",
      "Epoch 14 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.5059\n",
      "Epoch 14 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0856 0.5140\n",
      "Epoch 14 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.5139\n",
      "Epoch 14 Batch [220/300]\tTime 0.145 0.145\tLoss 1.8556 0.5209\n",
      "Epoch 14 Batch [230/300]\tTime 0.145 0.145\tLoss 0.0000 0.5169\n",
      "Epoch 14 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.5079\n",
      "Epoch 14 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.5021\n",
      "Epoch 14 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.4862\n",
      "Epoch 14 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.4686\n",
      "Epoch 14 Batch [280/300]\tTime 0.145 0.145\tLoss 3.7770 0.4794\n",
      "Epoch 14 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.4821\n",
      "Epoch 15 Batch [0/300]\tTime 0.156 0.156\tLoss 0.0000 0.0000\n",
      "Epoch 15 Batch [10/300]\tTime 0.144 0.151\tLoss 0.3711 0.1032\n",
      "Epoch 15 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.1864\n",
      "Epoch 15 Batch [30/300]\tTime 0.144 0.147\tLoss 0.0000 0.2519\n",
      "Epoch 15 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.3844\n",
      "Epoch 15 Batch [50/300]\tTime 0.148 0.146\tLoss 0.0000 0.4709\n",
      "Epoch 15 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.4775\n",
      "Epoch 15 Batch [70/300]\tTime 0.145 0.145\tLoss 0.0000 0.4103\n",
      "Epoch 15 Batch [80/300]\tTime 0.145 0.145\tLoss 0.0000 0.3898\n",
      "Epoch 15 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.4251\n",
      "Epoch 15 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.4127\n",
      "Epoch 15 Batch [110/300]\tTime 0.146 0.145\tLoss 0.0000 0.4057\n",
      "Epoch 15 Batch [120/300]\tTime 0.145 0.145\tLoss 0.8001 0.4135\n",
      "Epoch 15 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.3909\n",
      "Epoch 15 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.3757\n",
      "Epoch 15 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.3800\n",
      "Epoch 15 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.3944\n",
      "Epoch 15 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.4074\n",
      "Epoch 15 Batch [180/300]\tTime 0.145 0.145\tLoss 0.0000 0.3922\n",
      "Epoch 15 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.3787\n",
      "Epoch 15 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.3699\n",
      "Epoch 15 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.3590\n",
      "Epoch 15 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.3512\n",
      "Epoch 15 Batch [230/300]\tTime 0.145 0.145\tLoss 0.7437 0.3454\n",
      "Epoch 15 Batch [240/300]\tTime 0.148 0.145\tLoss 0.0000 0.3356\n",
      "Epoch 15 Batch [250/300]\tTime 0.145 0.145\tLoss 0.2147 0.3292\n",
      "Epoch 15 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.3202\n",
      "Epoch 15 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.3127\n",
      "Epoch 15 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.3111\n",
      "Epoch 15 Batch [290/300]\tTime 0.143 0.145\tLoss 0.1762 0.3053\n",
      "Epoch 16 Batch [0/300]\tTime 0.148 0.148\tLoss 4.5696 4.5696\n",
      "Epoch 16 Batch [10/300]\tTime 0.144 0.146\tLoss 0.0000 0.8171\n",
      "Epoch 16 Batch [20/300]\tTime 0.144 0.145\tLoss 0.0000 0.4422\n",
      "Epoch 16 Batch [30/300]\tTime 0.144 0.145\tLoss 0.0000 0.4773\n",
      "Epoch 16 Batch [40/300]\tTime 0.145 0.145\tLoss 0.6833 0.4016\n",
      "Epoch 16 Batch [50/300]\tTime 0.144 0.145\tLoss 0.9482 0.3529\n",
      "Epoch 16 Batch [60/300]\tTime 0.144 0.145\tLoss 0.0000 0.3875\n",
      "Epoch 16 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.3456\n",
      "Epoch 16 Batch [80/300]\tTime 0.144 0.145\tLoss 0.4794 0.3194\n",
      "Epoch 16 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.3053\n",
      "Epoch 16 Batch [100/300]\tTime 0.144 0.145\tLoss 0.7759 0.3215\n",
      "Epoch 16 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.3057\n",
      "Epoch 16 Batch [120/300]\tTime 0.145 0.145\tLoss 0.0000 0.2866\n",
      "Epoch 16 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.2797\n",
      "Epoch 16 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2859\n",
      "Epoch 16 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.2669\n",
      "Epoch 16 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2893\n",
      "Epoch 16 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.2874\n",
      "Epoch 16 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.2795\n",
      "Epoch 16 Batch [190/300]\tTime 0.145 0.145\tLoss 0.4052 0.2727\n",
      "Epoch 16 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.2852\n",
      "Epoch 16 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.2716\n",
      "Epoch 16 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.2677\n",
      "Epoch 16 Batch [230/300]\tTime 0.144 0.145\tLoss 3.2640 0.2815\n",
      "Epoch 16 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2796\n",
      "Epoch 16 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.2708\n",
      "Epoch 16 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2989\n",
      "Epoch 16 Batch [270/300]\tTime 0.145 0.145\tLoss 3.0981 0.2993\n",
      "Epoch 16 Batch [280/300]\tTime 0.146 0.145\tLoss 0.0000 0.2920\n",
      "Epoch 16 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.2855\n",
      "Epoch 17 Batch [0/300]\tTime 0.165 0.165\tLoss 0.0000 0.0000\n",
      "Epoch 17 Batch [10/300]\tTime 0.144 0.149\tLoss 0.0000 0.4102\n",
      "Epoch 17 Batch [20/300]\tTime 0.145 0.147\tLoss 0.3111 0.4534\n",
      "Epoch 17 Batch [30/300]\tTime 0.144 0.146\tLoss 1.0683 0.4267\n",
      "Epoch 17 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.3729\n",
      "Epoch 17 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.3849\n",
      "Epoch 17 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.3433\n",
      "Epoch 17 Batch [70/300]\tTime 0.145 0.146\tLoss 1.4523 0.3186\n",
      "Epoch 17 Batch [80/300]\tTime 0.144 0.145\tLoss 2.1154 0.3802\n",
      "Epoch 17 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.3472\n",
      "Epoch 17 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.3518\n",
      "Epoch 17 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.3353\n",
      "Epoch 17 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.3076\n",
      "Epoch 17 Batch [130/300]\tTime 0.145 0.145\tLoss 0.5013 0.2879\n",
      "Epoch 17 Batch [140/300]\tTime 0.144 0.145\tLoss 0.3005 0.3133\n",
      "Epoch 17 Batch [150/300]\tTime 0.144 0.145\tLoss 0.3740 0.3175\n",
      "Epoch 17 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0530 0.3004\n",
      "Epoch 17 Batch [170/300]\tTime 0.144 0.145\tLoss 0.7787 0.3093\n",
      "Epoch 17 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.3120\n",
      "Epoch 17 Batch [190/300]\tTime 0.145 0.145\tLoss 2.1929 0.3120\n",
      "Epoch 17 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.3166\n",
      "Epoch 17 Batch [210/300]\tTime 0.150 0.145\tLoss 0.0000 0.3056\n",
      "Epoch 17 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.3016\n",
      "Epoch 17 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.3008\n",
      "Epoch 17 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2992\n",
      "Epoch 17 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.2928\n",
      "Epoch 17 Batch [260/300]\tTime 0.145 0.145\tLoss 0.9982 0.2943\n",
      "Epoch 17 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2951\n",
      "Epoch 17 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.2870\n",
      "Epoch 17 Batch [290/300]\tTime 0.144 0.145\tLoss 0.4187 0.2908\n",
      "Epoch 18 Batch [0/300]\tTime 0.161 0.161\tLoss 0.0000 0.0000\n",
      "Epoch 18 Batch [10/300]\tTime 0.144 0.150\tLoss 0.0000 0.2840\n",
      "Epoch 18 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.2283\n",
      "Epoch 18 Batch [30/300]\tTime 0.144 0.147\tLoss 0.0000 0.2595\n",
      "Epoch 18 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.3297\n",
      "Epoch 18 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.3137\n",
      "Epoch 18 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.2702\n",
      "Epoch 18 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.2321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Batch [80/300]\tTime 0.145 0.145\tLoss 0.6910 0.2944\n",
      "Epoch 18 Batch [90/300]\tTime 0.144 0.145\tLoss 0.4432 0.3324\n",
      "Epoch 18 Batch [100/300]\tTime 0.146 0.145\tLoss 1.8214 0.3336\n",
      "Epoch 18 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.3276\n",
      "Epoch 18 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.3109\n",
      "Epoch 18 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.2965\n",
      "Epoch 18 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2885\n",
      "Epoch 18 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.2954\n",
      "Epoch 18 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2872\n",
      "Epoch 18 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.2836\n",
      "Epoch 18 Batch [180/300]\tTime 0.145 0.145\tLoss 1.0801 0.2792\n",
      "Epoch 18 Batch [190/300]\tTime 0.146 0.145\tLoss 0.0000 0.2805\n",
      "Epoch 18 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.2693\n",
      "Epoch 18 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.2618\n",
      "Epoch 18 Batch [220/300]\tTime 0.145 0.145\tLoss 0.6705 0.2877\n",
      "Epoch 18 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2818\n",
      "Epoch 18 Batch [240/300]\tTime 0.145 0.145\tLoss 2.5500 0.3018\n",
      "Epoch 18 Batch [250/300]\tTime 0.146 0.145\tLoss 1.0022 0.3004\n",
      "Epoch 18 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2903\n",
      "Epoch 18 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.3078\n",
      "Epoch 18 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.3066\n",
      "Epoch 18 Batch [290/300]\tTime 0.143 0.145\tLoss 0.3643 0.3049\n",
      "Epoch 19 Batch [0/300]\tTime 0.149 0.149\tLoss 0.0000 0.0000\n",
      "Epoch 19 Batch [10/300]\tTime 0.145 0.145\tLoss 0.0000 0.2271\n",
      "Epoch 19 Batch [20/300]\tTime 0.145 0.145\tLoss 0.2098 0.2349\n",
      "Epoch 19 Batch [30/300]\tTime 0.144 0.145\tLoss 0.0000 0.1858\n",
      "Epoch 19 Batch [40/300]\tTime 0.144 0.145\tLoss 0.0000 0.2952\n",
      "Epoch 19 Batch [50/300]\tTime 0.144 0.144\tLoss 0.0000 0.2793\n",
      "Epoch 19 Batch [60/300]\tTime 0.145 0.144\tLoss 0.0000 0.3310\n",
      "Epoch 19 Batch [70/300]\tTime 0.145 0.144\tLoss 0.0000 0.3100\n",
      "Epoch 19 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.3394\n",
      "Epoch 19 Batch [90/300]\tTime 0.144 0.145\tLoss 1.4881 0.3363\n",
      "Epoch 19 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.3487\n",
      "Epoch 19 Batch [110/300]\tTime 0.146 0.145\tLoss 0.0000 0.3329\n",
      "Epoch 19 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.3404\n",
      "Epoch 19 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.3279\n",
      "Epoch 19 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.3161\n",
      "Epoch 19 Batch [150/300]\tTime 0.144 0.145\tLoss 3.3206 0.3543\n",
      "Epoch 19 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.3371\n",
      "Epoch 19 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.3291\n",
      "Epoch 19 Batch [180/300]\tTime 0.145 0.145\tLoss 0.0000 0.3246\n",
      "Epoch 19 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.3230\n",
      "Epoch 19 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.3151\n",
      "Epoch 19 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.3254\n",
      "Epoch 19 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.3222\n",
      "Epoch 19 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.3082\n",
      "Epoch 19 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2974\n",
      "Epoch 19 Batch [250/300]\tTime 0.145 0.145\tLoss 5.1520 0.3121\n",
      "Epoch 19 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.3073\n",
      "Epoch 19 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.3016\n",
      "Epoch 19 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.2932\n",
      "Epoch 19 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2833\n",
      "Epoch 20 Batch [0/300]\tTime 0.168 0.168\tLoss 0.0000 0.0000\n",
      "Epoch 20 Batch [10/300]\tTime 0.148 0.149\tLoss 0.0000 0.2750\n",
      "Epoch 20 Batch [20/300]\tTime 0.145 0.147\tLoss 1.8512 0.2322\n",
      "Epoch 20 Batch [30/300]\tTime 0.144 0.146\tLoss 0.0000 0.1930\n",
      "Epoch 20 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.2541\n",
      "Epoch 20 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.2841\n",
      "Epoch 20 Batch [60/300]\tTime 0.145 0.146\tLoss 0.0000 0.2818\n",
      "Epoch 20 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.2665\n",
      "Epoch 20 Batch [80/300]\tTime 0.145 0.145\tLoss 0.0000 0.2549\n",
      "Epoch 20 Batch [90/300]\tTime 0.145 0.145\tLoss 0.0000 0.2294\n",
      "Epoch 20 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2218\n",
      "Epoch 20 Batch [110/300]\tTime 0.144 0.145\tLoss 1.4297 0.2414\n",
      "Epoch 20 Batch [120/300]\tTime 0.145 0.145\tLoss 0.0000 0.2489\n",
      "Epoch 20 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.2748\n",
      "Epoch 20 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.2553\n",
      "Epoch 20 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.2406\n",
      "Epoch 20 Batch [160/300]\tTime 0.146 0.145\tLoss 1.4619 0.2867\n",
      "Epoch 20 Batch [170/300]\tTime 0.146 0.145\tLoss 0.5894 0.2758\n",
      "Epoch 20 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.2696\n",
      "Epoch 20 Batch [190/300]\tTime 0.144 0.145\tLoss 1.5959 0.2687\n",
      "Epoch 20 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.2554\n",
      "Epoch 20 Batch [210/300]\tTime 0.144 0.145\tLoss 0.3530 0.2450\n",
      "Epoch 20 Batch [220/300]\tTime 0.144 0.145\tLoss 0.7095 0.2533\n",
      "Epoch 20 Batch [230/300]\tTime 0.144 0.145\tLoss 0.9011 0.2573\n",
      "Epoch 20 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2468\n",
      "Epoch 20 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.2436\n",
      "Epoch 20 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2515\n",
      "Epoch 20 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2500\n",
      "Epoch 20 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0996 0.2491\n",
      "Epoch 20 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2419\n",
      "Epoch 21 Batch [0/300]\tTime 0.165 0.165\tLoss 0.0000 0.0000\n",
      "Epoch 21 Batch [10/300]\tTime 0.145 0.151\tLoss 0.0000 0.2880\n",
      "Epoch 21 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.1742\n",
      "Epoch 21 Batch [30/300]\tTime 0.144 0.147\tLoss 1.3386 0.1612\n",
      "Epoch 21 Batch [40/300]\tTime 0.145 0.146\tLoss 0.0000 0.1548\n",
      "Epoch 21 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.1456\n",
      "Epoch 21 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.1412\n",
      "Epoch 21 Batch [70/300]\tTime 0.146 0.146\tLoss 0.0000 0.1346\n",
      "Epoch 21 Batch [80/300]\tTime 0.145 0.146\tLoss 1.0783 0.1855\n",
      "Epoch 21 Batch [90/300]\tTime 0.145 0.145\tLoss 0.0000 0.1682\n",
      "Epoch 21 Batch [100/300]\tTime 0.145 0.145\tLoss 0.0000 0.1838\n",
      "Epoch 21 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.1731\n",
      "Epoch 21 Batch [120/300]\tTime 0.145 0.145\tLoss 0.4921 0.1629\n",
      "Epoch 21 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1802\n",
      "Epoch 21 Batch [140/300]\tTime 0.145 0.145\tLoss 1.3553 0.1973\n",
      "Epoch 21 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1915\n",
      "Epoch 21 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1797\n",
      "Epoch 21 Batch [170/300]\tTime 0.145 0.145\tLoss 0.0000 0.1822\n",
      "Epoch 21 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1883\n",
      "Epoch 21 Batch [190/300]\tTime 0.148 0.145\tLoss 0.0000 0.1879\n",
      "Epoch 21 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1802\n",
      "Epoch 21 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1789\n",
      "Epoch 21 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1982\n",
      "Epoch 21 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2064\n",
      "Epoch 21 Batch [240/300]\tTime 0.145 0.145\tLoss 4.2760 0.2248\n",
      "Epoch 21 Batch [250/300]\tTime 0.145 0.145\tLoss 0.4970 0.2227\n",
      "Epoch 21 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.2211\n",
      "Epoch 21 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2180\n",
      "Epoch 21 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.2359\n",
      "Epoch 21 Batch [290/300]\tTime 0.145 0.145\tLoss 2.6336 0.2410\n",
      "Epoch 22 Batch [0/300]\tTime 0.149 0.149\tLoss 0.0000 0.0000\n",
      "Epoch 22 Batch [10/300]\tTime 0.144 0.154\tLoss 0.0000 0.6327\n",
      "Epoch 22 Batch [20/300]\tTime 0.144 0.150\tLoss 0.4614 0.4078\n",
      "Epoch 22 Batch [30/300]\tTime 0.144 0.148\tLoss 0.0000 0.2767\n",
      "Epoch 22 Batch [40/300]\tTime 0.145 0.147\tLoss 2.6601 0.4944\n",
      "Epoch 22 Batch [50/300]\tTime 0.144 0.147\tLoss 0.0000 0.5359\n",
      "Epoch 22 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.5345\n",
      "Epoch 22 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.5091\n",
      "Epoch 22 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.5011\n",
      "Epoch 22 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.4843\n",
      "Epoch 22 Batch [100/300]\tTime 0.145 0.146\tLoss 0.4714 0.4411\n",
      "Epoch 22 Batch [110/300]\tTime 0.145 0.146\tLoss 0.0000 0.4427\n",
      "Epoch 22 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0000 0.4061\n",
      "Epoch 22 Batch [130/300]\tTime 0.145 0.146\tLoss 0.0000 0.4176\n",
      "Epoch 22 Batch [140/300]\tTime 0.145 0.146\tLoss 0.0647 0.4159\n",
      "Epoch 22 Batch [150/300]\tTime 0.145 0.145\tLoss 0.9279 0.3948\n",
      "Epoch 22 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.3770\n",
      "Epoch 22 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.3777\n",
      "Epoch 22 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.3691\n",
      "Epoch 22 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.3588\n",
      "Epoch 22 Batch [200/300]\tTime 0.145 0.145\tLoss 2.0461 0.3512\n",
      "Epoch 22 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.3418\n",
      "Epoch 22 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.3275\n",
      "Epoch 22 Batch [230/300]\tTime 0.144 0.145\tLoss 2.0953 0.3232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.3187\n",
      "Epoch 22 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.3060\n",
      "Epoch 22 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.2993\n",
      "Epoch 22 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.2883\n",
      "Epoch 22 Batch [280/300]\tTime 0.144 0.145\tLoss 0.1489 0.2881\n",
      "Epoch 22 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2922\n",
      "Epoch 23 Batch [0/300]\tTime 0.155 0.155\tLoss 0.0000 0.0000\n",
      "Epoch 23 Batch [10/300]\tTime 0.144 0.145\tLoss 0.0000 0.0000\n",
      "Epoch 23 Batch [20/300]\tTime 0.146 0.145\tLoss 0.0000 0.1829\n",
      "Epoch 23 Batch [30/300]\tTime 0.144 0.145\tLoss 0.5616 0.3058\n",
      "Epoch 23 Batch [40/300]\tTime 0.144 0.145\tLoss 0.0000 0.2685\n",
      "Epoch 23 Batch [50/300]\tTime 0.144 0.145\tLoss 0.8678 0.2630\n",
      "Epoch 23 Batch [60/300]\tTime 0.145 0.145\tLoss 0.0000 0.2199\n",
      "Epoch 23 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.2007\n",
      "Epoch 23 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.1885\n",
      "Epoch 23 Batch [90/300]\tTime 0.145 0.145\tLoss 3.1773 0.2210\n",
      "Epoch 23 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2269\n",
      "Epoch 23 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.2436\n",
      "Epoch 23 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.2428\n",
      "Epoch 23 Batch [130/300]\tTime 0.151 0.145\tLoss 0.0000 0.2456\n",
      "Epoch 23 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2309\n",
      "Epoch 23 Batch [150/300]\tTime 0.146 0.145\tLoss 0.0000 0.2156\n",
      "Epoch 23 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2047\n",
      "Epoch 23 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1949\n",
      "Epoch 23 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1860\n",
      "Epoch 23 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0817 0.1872\n",
      "Epoch 23 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1929\n",
      "Epoch 23 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1936\n",
      "Epoch 23 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1888\n",
      "Epoch 23 Batch [230/300]\tTime 0.147 0.145\tLoss 2.2171 0.2263\n",
      "Epoch 23 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2169\n",
      "Epoch 23 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.2097\n",
      "Epoch 23 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2026\n",
      "Epoch 23 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.2053\n",
      "Epoch 23 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.2012\n",
      "Epoch 23 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1943\n",
      "Epoch 24 Batch [0/300]\tTime 0.162 0.162\tLoss 0.0000 0.0000\n",
      "Epoch 24 Batch [10/300]\tTime 0.144 0.152\tLoss 0.0000 0.5791\n",
      "Epoch 24 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.7046\n",
      "Epoch 24 Batch [30/300]\tTime 0.144 0.147\tLoss 0.0000 0.4773\n",
      "Epoch 24 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.4297\n",
      "Epoch 24 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.4048\n",
      "Epoch 24 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.3428\n",
      "Epoch 24 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.2958\n",
      "Epoch 24 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2593\n",
      "Epoch 24 Batch [90/300]\tTime 0.144 0.145\tLoss 1.1601 0.2970\n",
      "Epoch 24 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2761\n",
      "Epoch 24 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.3119\n",
      "Epoch 24 Batch [120/300]\tTime 0.147 0.145\tLoss 0.0000 0.2861\n",
      "Epoch 24 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.3004\n",
      "Epoch 24 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.2791\n",
      "Epoch 24 Batch [150/300]\tTime 0.145 0.145\tLoss 2.0181 0.2828\n",
      "Epoch 24 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2747\n",
      "Epoch 24 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.2909\n",
      "Epoch 24 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.2871\n",
      "Epoch 24 Batch [190/300]\tTime 0.147 0.145\tLoss 0.0000 0.2853\n",
      "Epoch 24 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.2974\n",
      "Epoch 24 Batch [210/300]\tTime 0.144 0.145\tLoss 1.6992 0.3036\n",
      "Epoch 24 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.2976\n",
      "Epoch 24 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2962\n",
      "Epoch 24 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2839\n",
      "Epoch 24 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.2855\n",
      "Epoch 24 Batch [260/300]\tTime 0.145 0.145\tLoss 1.9882 0.2822\n",
      "Epoch 24 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2827\n",
      "Epoch 24 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.2810\n",
      "Epoch 24 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2713\n",
      "Epoch 25 Batch [0/300]\tTime 0.159 0.159\tLoss 0.0000 0.0000\n",
      "Epoch 25 Batch [10/300]\tTime 0.144 0.148\tLoss 0.0000 0.0574\n",
      "Epoch 25 Batch [20/300]\tTime 0.145 0.147\tLoss 2.5106 0.2901\n",
      "Epoch 25 Batch [30/300]\tTime 0.144 0.146\tLoss 0.3360 0.2977\n",
      "Epoch 25 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.3301\n",
      "Epoch 25 Batch [50/300]\tTime 0.144 0.146\tLoss 0.6805 0.3205\n",
      "Epoch 25 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.2779\n",
      "Epoch 25 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.2387\n",
      "Epoch 25 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2757\n",
      "Epoch 25 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.2454\n",
      "Epoch 25 Batch [100/300]\tTime 0.145 0.145\tLoss 0.0000 0.2211\n",
      "Epoch 25 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.2192\n",
      "Epoch 25 Batch [120/300]\tTime 0.146 0.145\tLoss 0.0000 0.2638\n",
      "Epoch 25 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.2526\n",
      "Epoch 25 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.2347\n",
      "Epoch 25 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.2191\n",
      "Epoch 25 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2218\n",
      "Epoch 25 Batch [170/300]\tTime 0.145 0.145\tLoss 0.0000 0.2141\n",
      "Epoch 25 Batch [180/300]\tTime 0.144 0.145\tLoss 0.5639 0.2138\n",
      "Epoch 25 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.2126\n",
      "Epoch 25 Batch [200/300]\tTime 0.147 0.145\tLoss 0.0000 0.2109\n",
      "Epoch 25 Batch [210/300]\tTime 0.151 0.145\tLoss 0.0000 0.2009\n",
      "Epoch 25 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1954\n",
      "Epoch 25 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1903\n",
      "Epoch 25 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.1824\n",
      "Epoch 25 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1822\n",
      "Epoch 25 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1823\n",
      "Epoch 25 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1807\n",
      "Epoch 25 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.1872\n",
      "Epoch 25 Batch [290/300]\tTime 0.145 0.145\tLoss 0.0000 0.1865\n",
      "Epoch 26 Batch [0/300]\tTime 0.168 0.168\tLoss 0.0000 0.0000\n",
      "Epoch 26 Batch [10/300]\tTime 0.145 0.155\tLoss 0.0041 0.0004\n",
      "Epoch 26 Batch [20/300]\tTime 0.144 0.151\tLoss 0.0000 0.0816\n",
      "Epoch 26 Batch [30/300]\tTime 0.145 0.149\tLoss 0.0577 0.1612\n",
      "Epoch 26 Batch [40/300]\tTime 0.144 0.148\tLoss 0.0000 0.1484\n",
      "Epoch 26 Batch [50/300]\tTime 0.144 0.147\tLoss 1.1560 0.2028\n",
      "Epoch 26 Batch [60/300]\tTime 0.144 0.147\tLoss 0.0000 0.2473\n",
      "Epoch 26 Batch [70/300]\tTime 0.145 0.146\tLoss 3.4753 0.3415\n",
      "Epoch 26 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.3571\n",
      "Epoch 26 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.3738\n",
      "Epoch 26 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.3714\n",
      "Epoch 26 Batch [110/300]\tTime 0.145 0.146\tLoss 0.9972 0.3584\n",
      "Epoch 26 Batch [120/300]\tTime 0.145 0.146\tLoss 0.0000 0.3613\n",
      "Epoch 26 Batch [130/300]\tTime 0.145 0.146\tLoss 0.0000 0.3662\n",
      "Epoch 26 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.3553\n",
      "Epoch 26 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.3317\n",
      "Epoch 26 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.3214\n",
      "Epoch 26 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.3257\n",
      "Epoch 26 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.3414\n",
      "Epoch 26 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.3375\n",
      "Epoch 26 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.3243\n",
      "Epoch 26 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.3161\n",
      "Epoch 26 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.3431\n",
      "Epoch 26 Batch [230/300]\tTime 0.145 0.145\tLoss 0.0000 0.3290\n",
      "Epoch 26 Batch [240/300]\tTime 0.145 0.145\tLoss 2.4594 0.3258\n",
      "Epoch 26 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.3159\n",
      "Epoch 26 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.3237\n",
      "Epoch 26 Batch [270/300]\tTime 0.144 0.145\tLoss 0.4670 0.3217\n",
      "Epoch 26 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.3164\n",
      "Epoch 26 Batch [290/300]\tTime 0.143 0.145\tLoss 3.2942 0.3270\n",
      "Epoch 27 Batch [0/300]\tTime 0.171 0.171\tLoss 0.0000 0.0000\n",
      "Epoch 27 Batch [10/300]\tTime 0.144 0.151\tLoss 0.0000 0.1845\n",
      "Epoch 27 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.1506\n",
      "Epoch 27 Batch [30/300]\tTime 0.144 0.147\tLoss 1.4796 0.2014\n",
      "Epoch 27 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.2500\n",
      "Epoch 27 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.2370\n",
      "Epoch 27 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.2228\n",
      "Epoch 27 Batch [70/300]\tTime 0.145 0.146\tLoss 0.0000 0.2118\n",
      "Epoch 27 Batch [80/300]\tTime 0.145 0.145\tLoss 0.0000 0.2174\n",
      "Epoch 27 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.1935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.1762\n",
      "Epoch 27 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.1701\n",
      "Epoch 27 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1573\n",
      "Epoch 27 Batch [130/300]\tTime 0.145 0.145\tLoss 0.6886 0.1595\n",
      "Epoch 27 Batch [140/300]\tTime 0.144 0.145\tLoss 0.4763 0.1745\n",
      "Epoch 27 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1957\n",
      "Epoch 27 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1889\n",
      "Epoch 27 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1814\n",
      "Epoch 27 Batch [180/300]\tTime 0.144 0.145\tLoss 0.3603 0.1734\n",
      "Epoch 27 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.1672\n",
      "Epoch 27 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1787\n",
      "Epoch 27 Batch [210/300]\tTime 0.145 0.145\tLoss 0.0000 0.1704\n",
      "Epoch 27 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1627\n",
      "Epoch 27 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1660\n",
      "Epoch 27 Batch [240/300]\tTime 0.145 0.145\tLoss 1.5123 0.1712\n",
      "Epoch 27 Batch [250/300]\tTime 0.144 0.145\tLoss 1.3518 0.1697\n",
      "Epoch 27 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1632\n",
      "Epoch 27 Batch [270/300]\tTime 0.144 0.145\tLoss 1.0171 0.1610\n",
      "Epoch 27 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.1555\n",
      "Epoch 27 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1572\n",
      "Epoch 28 Batch [0/300]\tTime 0.172 0.172\tLoss 0.0000 0.0000\n",
      "Epoch 28 Batch [10/300]\tTime 0.145 0.159\tLoss 0.0000 0.0020\n",
      "Epoch 28 Batch [20/300]\tTime 0.144 0.152\tLoss 0.0000 0.0365\n",
      "Epoch 28 Batch [30/300]\tTime 0.144 0.150\tLoss 0.0000 0.0718\n",
      "Epoch 28 Batch [40/300]\tTime 0.144 0.148\tLoss 0.0000 0.0549\n",
      "Epoch 28 Batch [50/300]\tTime 0.144 0.148\tLoss 0.6920 0.1380\n",
      "Epoch 28 Batch [60/300]\tTime 0.144 0.147\tLoss 1.0030 0.1352\n",
      "Epoch 28 Batch [70/300]\tTime 0.144 0.147\tLoss 0.0000 0.1419\n",
      "Epoch 28 Batch [80/300]\tTime 0.144 0.147\tLoss 0.0000 0.1389\n",
      "Epoch 28 Batch [90/300]\tTime 0.144 0.147\tLoss 0.0000 0.1429\n",
      "Epoch 28 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.1518\n",
      "Epoch 28 Batch [110/300]\tTime 0.144 0.146\tLoss 0.2326 0.1751\n",
      "Epoch 28 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0000 0.1790\n",
      "Epoch 28 Batch [130/300]\tTime 0.145 0.146\tLoss 0.0000 0.2055\n",
      "Epoch 28 Batch [140/300]\tTime 0.145 0.146\tLoss 0.3022 0.2104\n",
      "Epoch 28 Batch [150/300]\tTime 0.144 0.146\tLoss 0.0000 0.2098\n",
      "Epoch 28 Batch [160/300]\tTime 0.145 0.146\tLoss 0.0000 0.2044\n",
      "Epoch 28 Batch [170/300]\tTime 0.148 0.146\tLoss 0.0000 0.1925\n",
      "Epoch 28 Batch [180/300]\tTime 0.144 0.146\tLoss 0.0000 0.2007\n",
      "Epoch 28 Batch [190/300]\tTime 0.144 0.146\tLoss 1.5023 0.2095\n",
      "Epoch 28 Batch [200/300]\tTime 0.144 0.146\tLoss 0.0000 0.2090\n",
      "Epoch 28 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.2119\n",
      "Epoch 28 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.2267\n",
      "Epoch 28 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2326\n",
      "Epoch 28 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2230\n",
      "Epoch 28 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.2251\n",
      "Epoch 28 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.2277\n",
      "Epoch 28 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2287\n",
      "Epoch 28 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.2359\n",
      "Epoch 28 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2281\n",
      "Epoch 29 Batch [0/300]\tTime 0.161 0.161\tLoss 0.0000 0.0000\n",
      "Epoch 29 Batch [10/300]\tTime 0.145 0.148\tLoss 0.1400 0.2077\n",
      "Epoch 29 Batch [20/300]\tTime 0.145 0.146\tLoss 0.0000 0.1736\n",
      "Epoch 29 Batch [30/300]\tTime 0.149 0.146\tLoss 1.5086 0.2114\n",
      "Epoch 29 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.1923\n",
      "Epoch 29 Batch [50/300]\tTime 0.146 0.146\tLoss 0.0000 0.2107\n",
      "Epoch 29 Batch [60/300]\tTime 0.145 0.145\tLoss 0.0000 0.1920\n",
      "Epoch 29 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.1989\n",
      "Epoch 29 Batch [80/300]\tTime 0.145 0.145\tLoss 0.0000 0.2305\n",
      "Epoch 29 Batch [90/300]\tTime 0.145 0.145\tLoss 0.1733 0.2159\n",
      "Epoch 29 Batch [100/300]\tTime 0.145 0.145\tLoss 0.0000 0.2086\n",
      "Epoch 29 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.1908\n",
      "Epoch 29 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1932\n",
      "Epoch 29 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.1800\n",
      "Epoch 29 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1799\n",
      "Epoch 29 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1946\n",
      "Epoch 29 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2073\n",
      "Epoch 29 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1952\n",
      "Epoch 29 Batch [180/300]\tTime 0.145 0.145\tLoss 2.4901 0.2008\n",
      "Epoch 29 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.1989\n",
      "Epoch 29 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1993\n",
      "Epoch 29 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1898\n",
      "Epoch 29 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1812\n",
      "Epoch 29 Batch [230/300]\tTime 0.145 0.145\tLoss 0.0000 0.1770\n",
      "Epoch 29 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1876\n",
      "Epoch 29 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1801\n",
      "Epoch 29 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.1766\n",
      "Epoch 29 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1754\n",
      "Epoch 29 Batch [280/300]\tTime 0.145 0.145\tLoss 0.6639 0.1826\n",
      "Epoch 29 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1763\n",
      "Epoch 30 Batch [0/300]\tTime 0.168 0.168\tLoss 0.0000 0.0000\n",
      "Epoch 30 Batch [10/300]\tTime 0.144 0.157\tLoss 0.0000 0.3517\n",
      "Epoch 30 Batch [20/300]\tTime 0.145 0.151\tLoss 0.0000 0.3061\n",
      "Epoch 30 Batch [30/300]\tTime 0.145 0.149\tLoss 1.1542 0.2446\n",
      "Epoch 30 Batch [40/300]\tTime 0.145 0.148\tLoss 3.5263 0.2788\n",
      "Epoch 30 Batch [50/300]\tTime 0.144 0.147\tLoss 0.0000 0.2369\n",
      "Epoch 30 Batch [60/300]\tTime 0.146 0.147\tLoss 0.0000 0.2112\n",
      "Epoch 30 Batch [70/300]\tTime 0.144 0.147\tLoss 0.0000 0.2705\n",
      "Epoch 30 Batch [80/300]\tTime 0.154 0.147\tLoss 0.2255 0.2399\n",
      "Epoch 30 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.2238\n",
      "Epoch 30 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.2248\n",
      "Epoch 30 Batch [110/300]\tTime 0.144 0.146\tLoss 0.0000 0.2288\n",
      "Epoch 30 Batch [120/300]\tTime 0.148 0.146\tLoss 0.0000 0.2099\n",
      "Epoch 30 Batch [130/300]\tTime 0.144 0.146\tLoss 0.0000 0.2072\n",
      "Epoch 30 Batch [140/300]\tTime 0.144 0.146\tLoss 1.8514 0.2481\n",
      "Epoch 30 Batch [150/300]\tTime 0.144 0.146\tLoss 0.3435 0.2385\n",
      "Epoch 30 Batch [160/300]\tTime 0.144 0.146\tLoss 0.0000 0.2280\n",
      "Epoch 30 Batch [170/300]\tTime 0.144 0.146\tLoss 0.0000 0.2486\n",
      "Epoch 30 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.2374\n",
      "Epoch 30 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.2275\n",
      "Epoch 30 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.2464\n",
      "Epoch 30 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.2448\n",
      "Epoch 30 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.2385\n",
      "Epoch 30 Batch [230/300]\tTime 0.145 0.145\tLoss 2.4412 0.2427\n",
      "Epoch 30 Batch [240/300]\tTime 0.144 0.145\tLoss 1.4467 0.2450\n",
      "Epoch 30 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.2387\n",
      "Epoch 30 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2299\n",
      "Epoch 30 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2288\n",
      "Epoch 30 Batch [280/300]\tTime 0.144 0.145\tLoss 1.2573 0.2252\n",
      "Epoch 30 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2241\n",
      "Epoch 31 Batch [0/300]\tTime 0.156 0.156\tLoss 0.0000 0.0000\n",
      "Epoch 31 Batch [10/300]\tTime 0.144 0.153\tLoss 0.0000 0.0121\n",
      "Epoch 31 Batch [20/300]\tTime 0.144 0.149\tLoss 0.0000 0.0349\n",
      "Epoch 31 Batch [30/300]\tTime 0.145 0.148\tLoss 0.0000 0.1532\n",
      "Epoch 31 Batch [40/300]\tTime 0.145 0.147\tLoss 0.2346 0.2574\n",
      "Epoch 31 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.2069\n",
      "Epoch 31 Batch [60/300]\tTime 0.145 0.146\tLoss 0.0000 0.1940\n",
      "Epoch 31 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.1794\n",
      "Epoch 31 Batch [80/300]\tTime 0.145 0.146\tLoss 0.0000 0.1839\n",
      "Epoch 31 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.1639\n",
      "Epoch 31 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.1891\n",
      "Epoch 31 Batch [110/300]\tTime 0.146 0.145\tLoss 0.0000 0.1863\n",
      "Epoch 31 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1992\n",
      "Epoch 31 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.2123\n",
      "Epoch 31 Batch [140/300]\tTime 0.144 0.145\tLoss 0.6216 0.2072\n",
      "Epoch 31 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1934\n",
      "Epoch 31 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1856\n",
      "Epoch 31 Batch [170/300]\tTime 0.147 0.145\tLoss 0.0000 0.1846\n",
      "Epoch 31 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1758\n",
      "Epoch 31 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.1724\n",
      "Epoch 31 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.1639\n",
      "Epoch 31 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1655\n",
      "Epoch 31 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1592\n",
      "Epoch 31 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1574\n",
      "Epoch 31 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1537\n",
      "Epoch 31 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1479\n",
      "Epoch 31 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.1458\n",
      "Epoch 31 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.1406\n",
      "Epoch 31 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.1403\n",
      "Epoch 32 Batch [0/300]\tTime 0.171 0.171\tLoss 0.0000 0.0000\n",
      "Epoch 32 Batch [10/300]\tTime 0.145 0.152\tLoss 0.0000 0.1268\n",
      "Epoch 32 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.1666\n",
      "Epoch 32 Batch [30/300]\tTime 0.145 0.147\tLoss 1.9245 0.2759\n",
      "Epoch 32 Batch [40/300]\tTime 0.144 0.146\tLoss 2.6385 0.2979\n",
      "Epoch 32 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.2395\n",
      "Epoch 32 Batch [60/300]\tTime 0.145 0.146\tLoss 0.0000 0.2060\n",
      "Epoch 32 Batch [70/300]\tTime 0.145 0.146\tLoss 0.0000 0.1891\n",
      "Epoch 32 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.1733\n",
      "Epoch 32 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.1584\n",
      "Epoch 32 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.1473\n",
      "Epoch 32 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.1503\n",
      "Epoch 32 Batch [120/300]\tTime 0.145 0.145\tLoss 0.0000 0.1379\n",
      "Epoch 32 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1273\n",
      "Epoch 32 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1261\n",
      "Epoch 32 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.1553\n",
      "Epoch 32 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.1656\n",
      "Epoch 32 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1730\n",
      "Epoch 32 Batch [180/300]\tTime 0.145 0.145\tLoss 1.2947 0.1741\n",
      "Epoch 32 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.1664\n",
      "Epoch 32 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1590\n",
      "Epoch 32 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1589\n",
      "Epoch 32 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1674\n",
      "Epoch 32 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1645\n",
      "Epoch 32 Batch [240/300]\tTime 0.147 0.145\tLoss 0.0000 0.1577\n",
      "Epoch 32 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1593\n",
      "Epoch 32 Batch [260/300]\tTime 0.147 0.145\tLoss 1.3945 0.1923\n",
      "Epoch 32 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1852\n",
      "Epoch 32 Batch [280/300]\tTime 0.145 0.145\tLoss 0.1772 0.1822\n",
      "Epoch 32 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1852\n",
      "Epoch 33 Batch [0/300]\tTime 0.181 0.181\tLoss 0.0629 0.0629\n",
      "Epoch 33 Batch [10/300]\tTime 0.145 0.151\tLoss 2.5687 0.5018\n",
      "Epoch 33 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.3564\n",
      "Epoch 33 Batch [30/300]\tTime 0.145 0.147\tLoss 0.0000 0.2414\n",
      "Epoch 33 Batch [40/300]\tTime 0.145 0.147\tLoss 0.0000 0.2491\n",
      "Epoch 33 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.2346\n",
      "Epoch 33 Batch [60/300]\tTime 0.145 0.146\tLoss 0.0000 0.2065\n",
      "Epoch 33 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.2082\n",
      "Epoch 33 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.1999\n",
      "Epoch 33 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.2104\n",
      "Epoch 33 Batch [100/300]\tTime 0.148 0.145\tLoss 2.0314 0.2096\n",
      "Epoch 33 Batch [110/300]\tTime 0.144 0.145\tLoss 0.6376 0.1987\n",
      "Epoch 33 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1947\n",
      "Epoch 33 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1823\n",
      "Epoch 33 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.1947\n",
      "Epoch 33 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1825\n",
      "Epoch 33 Batch [160/300]\tTime 0.145 0.145\tLoss 1.3742 0.1941\n",
      "Epoch 33 Batch [170/300]\tTime 0.146 0.145\tLoss 0.0000 0.1866\n",
      "Epoch 33 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1789\n",
      "Epoch 33 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.1695\n",
      "Epoch 33 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1749\n",
      "Epoch 33 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1666\n",
      "Epoch 33 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.1645\n",
      "Epoch 33 Batch [230/300]\tTime 0.144 0.145\tLoss 0.9510 0.1643\n",
      "Epoch 33 Batch [240/300]\tTime 0.145 0.145\tLoss 0.2418 0.1714\n",
      "Epoch 33 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1646\n",
      "Epoch 33 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1707\n",
      "Epoch 33 Batch [270/300]\tTime 0.145 0.145\tLoss 0.9440 0.1720\n",
      "Epoch 33 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.1708\n",
      "Epoch 33 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.1770\n",
      "Epoch 34 Batch [0/300]\tTime 0.180 0.180\tLoss 2.1328 2.1328\n",
      "Epoch 34 Batch [10/300]\tTime 0.145 0.153\tLoss 0.0000 0.2002\n",
      "Epoch 34 Batch [20/300]\tTime 0.144 0.149\tLoss 0.0000 0.1795\n",
      "Epoch 34 Batch [30/300]\tTime 0.145 0.148\tLoss 0.0000 0.2471\n",
      "Epoch 34 Batch [40/300]\tTime 0.144 0.147\tLoss 0.0000 0.2230\n",
      "Epoch 34 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.2411\n",
      "Epoch 34 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.2016\n",
      "Epoch 34 Batch [70/300]\tTime 0.145 0.146\tLoss 0.0000 0.2067\n",
      "Epoch 34 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.2083\n",
      "Epoch 34 Batch [90/300]\tTime 0.145 0.146\tLoss 0.0000 0.2213\n",
      "Epoch 34 Batch [100/300]\tTime 0.145 0.145\tLoss 0.0000 0.2364\n",
      "Epoch 34 Batch [110/300]\tTime 0.145 0.145\tLoss 1.4508 0.2474\n",
      "Epoch 34 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.2361\n",
      "Epoch 34 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.2305\n",
      "Epoch 34 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2404\n",
      "Epoch 34 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.2415\n",
      "Epoch 34 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.2287\n",
      "Epoch 34 Batch [170/300]\tTime 0.151 0.145\tLoss 0.0000 0.2205\n",
      "Epoch 34 Batch [180/300]\tTime 0.146 0.145\tLoss 0.0000 0.2123\n",
      "Epoch 34 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.2037\n",
      "Epoch 34 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.2126\n",
      "Epoch 34 Batch [210/300]\tTime 0.144 0.145\tLoss 0.6645 0.2073\n",
      "Epoch 34 Batch [220/300]\tTime 0.145 0.145\tLoss 1.4675 0.2268\n",
      "Epoch 34 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2191\n",
      "Epoch 34 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2100\n",
      "Epoch 34 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.2457\n",
      "Epoch 34 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.2549\n",
      "Epoch 34 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.2526\n",
      "Epoch 34 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.2512\n",
      "Epoch 34 Batch [290/300]\tTime 0.143 0.145\tLoss 0.2260 0.2479\n",
      "Epoch 35 Batch [0/300]\tTime 0.170 0.170\tLoss 0.0000 0.0000\n",
      "Epoch 35 Batch [10/300]\tTime 0.149 0.149\tLoss 0.0000 0.2589\n",
      "Epoch 35 Batch [20/300]\tTime 0.144 0.147\tLoss 0.0000 0.1739\n",
      "Epoch 35 Batch [30/300]\tTime 0.147 0.146\tLoss 0.0000 0.1223\n",
      "Epoch 35 Batch [40/300]\tTime 0.144 0.146\tLoss 1.5370 0.2007\n",
      "Epoch 35 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.2065\n",
      "Epoch 35 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.2454\n",
      "Epoch 35 Batch [70/300]\tTime 0.145 0.145\tLoss 0.0000 0.3334\n",
      "Epoch 35 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2923\n",
      "Epoch 35 Batch [90/300]\tTime 0.146 0.145\tLoss 0.0000 0.3151\n",
      "Epoch 35 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.3154\n",
      "Epoch 35 Batch [110/300]\tTime 0.153 0.145\tLoss 0.0000 0.3083\n",
      "Epoch 35 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.3005\n",
      "Epoch 35 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.2861\n",
      "Epoch 35 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2772\n",
      "Epoch 35 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.2648\n",
      "Epoch 35 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.2501\n",
      "Epoch 35 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.2450\n",
      "Epoch 35 Batch [180/300]\tTime 0.144 0.145\tLoss 2.7278 0.2515\n",
      "Epoch 35 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.2439\n",
      "Epoch 35 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.2406\n",
      "Epoch 35 Batch [210/300]\tTime 0.145 0.145\tLoss 0.0000 0.2585\n",
      "Epoch 35 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.2468\n",
      "Epoch 35 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2421\n",
      "Epoch 35 Batch [240/300]\tTime 0.145 0.145\tLoss 0.1314 0.2373\n",
      "Epoch 35 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.2282\n",
      "Epoch 35 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2273\n",
      "Epoch 35 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.2419\n",
      "Epoch 35 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.2333\n",
      "Epoch 35 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.2449\n",
      "Epoch 36 Batch [0/300]\tTime 0.163 0.163\tLoss 0.0000 0.0000\n",
      "Epoch 36 Batch [10/300]\tTime 0.147 0.158\tLoss 0.0000 0.0000\n",
      "Epoch 36 Batch [20/300]\tTime 0.145 0.152\tLoss 0.0000 0.0337\n",
      "Epoch 36 Batch [30/300]\tTime 0.144 0.149\tLoss 0.0000 0.1565\n",
      "Epoch 36 Batch [40/300]\tTime 0.144 0.148\tLoss 0.0000 0.1183\n",
      "Epoch 36 Batch [50/300]\tTime 0.144 0.147\tLoss 0.0000 0.0951\n",
      "Epoch 36 Batch [60/300]\tTime 0.144 0.147\tLoss 0.0000 0.1314\n",
      "Epoch 36 Batch [70/300]\tTime 0.145 0.147\tLoss 0.0000 0.1224\n",
      "Epoch 36 Batch [80/300]\tTime 0.145 0.146\tLoss 0.0000 0.1486\n",
      "Epoch 36 Batch [90/300]\tTime 0.145 0.146\tLoss 0.0000 0.1907\n",
      "Epoch 36 Batch [100/300]\tTime 0.145 0.146\tLoss 0.0000 0.1718\n",
      "Epoch 36 Batch [110/300]\tTime 0.144 0.146\tLoss 0.6455 0.1651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36 Batch [120/300]\tTime 0.145 0.146\tLoss 0.0000 0.1633\n",
      "Epoch 36 Batch [130/300]\tTime 0.144 0.146\tLoss 0.0000 0.1509\n",
      "Epoch 36 Batch [140/300]\tTime 0.144 0.146\tLoss 0.0000 0.1445\n",
      "Epoch 36 Batch [150/300]\tTime 0.145 0.146\tLoss 0.0000 0.1366\n",
      "Epoch 36 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1331\n",
      "Epoch 36 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1337\n",
      "Epoch 36 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1276\n",
      "Epoch 36 Batch [190/300]\tTime 0.148 0.145\tLoss 0.0000 0.1354\n",
      "Epoch 36 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1440\n",
      "Epoch 36 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1511\n",
      "Epoch 36 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1490\n",
      "Epoch 36 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1425\n",
      "Epoch 36 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1450\n",
      "Epoch 36 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1407\n",
      "Epoch 36 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1464\n",
      "Epoch 36 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1452\n",
      "Epoch 36 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.1490\n",
      "Epoch 36 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1443\n",
      "Epoch 37 Batch [0/300]\tTime 0.169 0.169\tLoss 0.0000 0.0000\n",
      "Epoch 37 Batch [10/300]\tTime 0.145 0.150\tLoss 0.0000 0.0005\n",
      "Epoch 37 Batch [20/300]\tTime 0.145 0.148\tLoss 3.6652 0.4222\n",
      "Epoch 37 Batch [30/300]\tTime 0.144 0.147\tLoss 0.0000 0.3379\n",
      "Epoch 37 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.2555\n",
      "Epoch 37 Batch [50/300]\tTime 0.144 0.146\tLoss 0.4017 0.4059\n",
      "Epoch 37 Batch [60/300]\tTime 0.144 0.146\tLoss 0.2174 0.3749\n",
      "Epoch 37 Batch [70/300]\tTime 0.144 0.145\tLoss 0.6777 0.3346\n",
      "Epoch 37 Batch [80/300]\tTime 0.145 0.145\tLoss 0.0000 0.3445\n",
      "Epoch 37 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.4021\n",
      "Epoch 37 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.4032\n",
      "Epoch 37 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.4148\n",
      "Epoch 37 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.4149\n",
      "Epoch 37 Batch [130/300]\tTime 0.144 0.145\tLoss 1.5081 0.3973\n",
      "Epoch 37 Batch [140/300]\tTime 0.145 0.145\tLoss 3.4922 0.3939\n",
      "Epoch 37 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.3762\n",
      "Epoch 37 Batch [160/300]\tTime 0.150 0.145\tLoss 0.5997 0.3687\n",
      "Epoch 37 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0276 0.3526\n",
      "Epoch 37 Batch [180/300]\tTime 0.145 0.145\tLoss 0.6002 0.3581\n",
      "Epoch 37 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.3452\n",
      "Epoch 37 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.3442\n",
      "Epoch 37 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.3361\n",
      "Epoch 37 Batch [220/300]\tTime 0.145 0.145\tLoss 2.2534 0.3388\n",
      "Epoch 37 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.3283\n",
      "Epoch 37 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.3147\n",
      "Epoch 37 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.3022\n",
      "Epoch 37 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2919\n",
      "Epoch 37 Batch [270/300]\tTime 0.145 0.145\tLoss 3.3278 0.3059\n",
      "Epoch 37 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.3013\n",
      "Epoch 37 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.2941\n",
      "Epoch 38 Batch [0/300]\tTime 0.155 0.155\tLoss 0.0000 0.0000\n",
      "Epoch 38 Batch [10/300]\tTime 0.149 0.150\tLoss 0.0000 0.0292\n",
      "Epoch 38 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.0153\n",
      "Epoch 38 Batch [30/300]\tTime 0.144 0.147\tLoss 1.0186 0.0432\n",
      "Epoch 38 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.1376\n",
      "Epoch 38 Batch [50/300]\tTime 0.145 0.146\tLoss 0.2971 0.1770\n",
      "Epoch 38 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.1480\n",
      "Epoch 38 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.1271\n",
      "Epoch 38 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.1554\n",
      "Epoch 38 Batch [90/300]\tTime 0.145 0.145\tLoss 0.0000 0.1566\n",
      "Epoch 38 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.1411\n",
      "Epoch 38 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.1416\n",
      "Epoch 38 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1332\n",
      "Epoch 38 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1303\n",
      "Epoch 38 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1211\n",
      "Epoch 38 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1192\n",
      "Epoch 38 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1240\n",
      "Epoch 38 Batch [170/300]\tTime 0.146 0.145\tLoss 0.1908 0.1178\n",
      "Epoch 38 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1218\n",
      "Epoch 38 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.1382\n",
      "Epoch 38 Batch [200/300]\tTime 0.145 0.145\tLoss 1.7652 0.1401\n",
      "Epoch 38 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1421\n",
      "Epoch 38 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1361\n",
      "Epoch 38 Batch [230/300]\tTime 0.145 0.145\tLoss 0.0000 0.1516\n",
      "Epoch 38 Batch [240/300]\tTime 0.144 0.145\tLoss 0.4895 0.1473\n",
      "Epoch 38 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1442\n",
      "Epoch 38 Batch [260/300]\tTime 0.145 0.145\tLoss 0.0000 0.1386\n",
      "Epoch 38 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1552\n",
      "Epoch 38 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.1589\n",
      "Epoch 38 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.1587\n",
      "Epoch 39 Batch [0/300]\tTime 0.148 0.148\tLoss 0.0000 0.0000\n",
      "Epoch 39 Batch [10/300]\tTime 0.144 0.145\tLoss 0.0000 0.1404\n",
      "Epoch 39 Batch [20/300]\tTime 0.144 0.145\tLoss 0.0000 0.1194\n",
      "Epoch 39 Batch [30/300]\tTime 0.144 0.145\tLoss 0.0000 0.1068\n",
      "Epoch 39 Batch [40/300]\tTime 0.144 0.145\tLoss 0.0000 0.0807\n",
      "Epoch 39 Batch [50/300]\tTime 0.144 0.145\tLoss 0.0000 0.1383\n",
      "Epoch 39 Batch [60/300]\tTime 0.144 0.145\tLoss 0.0000 0.1698\n",
      "Epoch 39 Batch [70/300]\tTime 0.144 0.145\tLoss 0.1312 0.2180\n",
      "Epoch 39 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2489\n",
      "Epoch 39 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.2292\n",
      "Epoch 39 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2573\n",
      "Epoch 39 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.2482\n",
      "Epoch 39 Batch [120/300]\tTime 0.145 0.145\tLoss 0.0000 0.2673\n",
      "Epoch 39 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.3041\n",
      "Epoch 39 Batch [140/300]\tTime 0.145 0.145\tLoss 0.0000 0.3719\n",
      "Epoch 39 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.3548\n",
      "Epoch 39 Batch [160/300]\tTime 0.145 0.145\tLoss 0.7407 0.3673\n",
      "Epoch 39 Batch [170/300]\tTime 0.146 0.145\tLoss 0.0000 0.3458\n",
      "Epoch 39 Batch [180/300]\tTime 0.145 0.145\tLoss 0.0000 0.3432\n",
      "Epoch 39 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.3272\n",
      "Epoch 39 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.3420\n",
      "Epoch 39 Batch [210/300]\tTime 0.146 0.145\tLoss 1.6546 0.3336\n",
      "Epoch 39 Batch [220/300]\tTime 0.145 0.145\tLoss 1.7936 0.3345\n",
      "Epoch 39 Batch [230/300]\tTime 0.145 0.145\tLoss 0.0000 0.3274\n",
      "Epoch 39 Batch [240/300]\tTime 0.145 0.145\tLoss 3.1844 0.3348\n",
      "Epoch 39 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.3306\n",
      "Epoch 39 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.3197\n",
      "Epoch 39 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.3219\n",
      "Epoch 39 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.3169\n",
      "Epoch 39 Batch [290/300]\tTime 0.146 0.145\tLoss 0.0000 0.3267\n",
      "Epoch 40 Batch [0/300]\tTime 0.169 0.169\tLoss 0.0000 0.0000\n",
      "Epoch 40 Batch [10/300]\tTime 0.145 0.149\tLoss 0.0000 0.1530\n",
      "Epoch 40 Batch [20/300]\tTime 0.144 0.147\tLoss 0.0000 0.1960\n",
      "Epoch 40 Batch [30/300]\tTime 0.144 0.146\tLoss 0.0000 0.1328\n",
      "Epoch 40 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.1709\n",
      "Epoch 40 Batch [50/300]\tTime 0.144 0.146\tLoss 1.7652 0.2566\n",
      "Epoch 40 Batch [60/300]\tTime 0.144 0.145\tLoss 0.0000 0.2433\n",
      "Epoch 40 Batch [70/300]\tTime 0.145 0.145\tLoss 0.0000 0.2417\n",
      "Epoch 40 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2534\n",
      "Epoch 40 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.2256\n",
      "Epoch 40 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2065\n",
      "Epoch 40 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.2244\n",
      "Epoch 40 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.2089\n",
      "Epoch 40 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.2462\n",
      "Epoch 40 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.2730\n",
      "Epoch 40 Batch [150/300]\tTime 0.148 0.145\tLoss 0.0000 0.2983\n",
      "Epoch 40 Batch [160/300]\tTime 0.144 0.145\tLoss 1.8746 0.3035\n",
      "Epoch 40 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.2875\n",
      "Epoch 40 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.3003\n",
      "Epoch 40 Batch [190/300]\tTime 0.145 0.145\tLoss 1.5837 0.2929\n",
      "Epoch 40 Batch [200/300]\tTime 0.145 0.145\tLoss 0.2439 0.2936\n",
      "Epoch 40 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.2909\n",
      "Epoch 40 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.2777\n",
      "Epoch 40 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2992\n",
      "Epoch 40 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2884\n",
      "Epoch 40 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.2964\n",
      "Epoch 40 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2902\n",
      "Epoch 40 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.2783\n",
      "Epoch 40 Batch [290/300]\tTime 0.143 0.145\tLoss 0.8333 0.2743\n",
      "Epoch 41 Batch [0/300]\tTime 0.153 0.153\tLoss 0.0000 0.0000\n",
      "Epoch 41 Batch [10/300]\tTime 0.144 0.150\tLoss 0.0000 0.1806\n",
      "Epoch 41 Batch [20/300]\tTime 0.145 0.147\tLoss 0.0000 0.0946\n",
      "Epoch 41 Batch [30/300]\tTime 0.145 0.146\tLoss 1.2581 0.1441\n",
      "Epoch 41 Batch [40/300]\tTime 0.144 0.146\tLoss 0.0000 0.1273\n",
      "Epoch 41 Batch [50/300]\tTime 0.145 0.146\tLoss 0.0000 0.1997\n",
      "Epoch 41 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.2118\n",
      "Epoch 41 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.1938\n",
      "Epoch 41 Batch [80/300]\tTime 0.144 0.146\tLoss 1.6165 0.2025\n",
      "Epoch 41 Batch [90/300]\tTime 0.144 0.146\tLoss 0.7842 0.2009\n",
      "Epoch 41 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.1810\n",
      "Epoch 41 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.1647\n",
      "Epoch 41 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0000 0.1607\n",
      "Epoch 41 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1556\n",
      "Epoch 41 Batch [140/300]\tTime 0.144 0.145\tLoss 0.6022 0.1586\n",
      "Epoch 41 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.1663\n",
      "Epoch 41 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1616\n",
      "Epoch 41 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1625\n",
      "Epoch 41 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1536\n",
      "Epoch 41 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.1455\n",
      "Epoch 41 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.1501\n",
      "Epoch 41 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1430\n",
      "Epoch 41 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.1458\n",
      "Epoch 41 Batch [230/300]\tTime 0.145 0.145\tLoss 2.8928 0.1594\n",
      "Epoch 41 Batch [240/300]\tTime 0.146 0.145\tLoss 0.0000 0.1575\n",
      "Epoch 41 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1591\n",
      "Epoch 41 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1530\n",
      "Epoch 41 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1512\n",
      "Epoch 41 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.1490\n",
      "Epoch 41 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1463\n",
      "Epoch 42 Batch [0/300]\tTime 0.179 0.179\tLoss 0.0000 0.0000\n",
      "Epoch 42 Batch [10/300]\tTime 0.144 0.154\tLoss 0.0000 0.0000\n",
      "Epoch 42 Batch [20/300]\tTime 0.144 0.150\tLoss 0.0000 0.1686\n",
      "Epoch 42 Batch [30/300]\tTime 0.145 0.148\tLoss 0.0000 0.1379\n",
      "Epoch 42 Batch [40/300]\tTime 0.144 0.147\tLoss 0.0000 0.1042\n",
      "Epoch 42 Batch [50/300]\tTime 0.144 0.147\tLoss 0.0000 0.1160\n",
      "Epoch 42 Batch [60/300]\tTime 0.145 0.146\tLoss 1.6324 0.1532\n",
      "Epoch 42 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.1635\n",
      "Epoch 42 Batch [80/300]\tTime 0.144 0.146\tLoss 0.4049 0.1797\n",
      "Epoch 42 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.2092\n",
      "Epoch 42 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.2186\n",
      "Epoch 42 Batch [110/300]\tTime 0.144 0.146\tLoss 0.0000 0.2067\n",
      "Epoch 42 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.2061\n",
      "Epoch 42 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.1983\n",
      "Epoch 42 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1843\n",
      "Epoch 42 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1760\n",
      "Epoch 42 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.1695\n",
      "Epoch 42 Batch [170/300]\tTime 0.146 0.145\tLoss 0.1665 0.1615\n",
      "Epoch 42 Batch [180/300]\tTime 0.145 0.145\tLoss 0.0000 0.1530\n",
      "Epoch 42 Batch [190/300]\tTime 0.145 0.145\tLoss 0.3886 0.1603\n",
      "Epoch 42 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1530\n",
      "Epoch 42 Batch [210/300]\tTime 0.145 0.145\tLoss 0.0000 0.1637\n",
      "Epoch 42 Batch [220/300]\tTime 0.152 0.145\tLoss 0.0000 0.1563\n",
      "Epoch 42 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1496\n",
      "Epoch 42 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1598\n",
      "Epoch 42 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1624\n",
      "Epoch 42 Batch [260/300]\tTime 0.152 0.145\tLoss 0.0000 0.1562\n",
      "Epoch 42 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1616\n",
      "Epoch 42 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.1630\n",
      "Epoch 42 Batch [290/300]\tTime 0.144 0.145\tLoss 0.2475 0.1700\n",
      "Epoch 43 Batch [0/300]\tTime 0.157 0.157\tLoss 0.0000 0.0000\n",
      "Epoch 43 Batch [10/300]\tTime 0.145 0.148\tLoss 0.0000 0.4089\n",
      "Epoch 43 Batch [20/300]\tTime 0.145 0.146\tLoss 0.0000 0.2811\n",
      "Epoch 43 Batch [30/300]\tTime 0.145 0.146\tLoss 0.0000 0.2233\n",
      "Epoch 43 Batch [40/300]\tTime 0.145 0.146\tLoss 0.0000 0.1688\n",
      "Epoch 43 Batch [50/300]\tTime 0.145 0.145\tLoss 0.0000 0.1623\n",
      "Epoch 43 Batch [60/300]\tTime 0.145 0.145\tLoss 0.0000 0.1747\n",
      "Epoch 43 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.1764\n",
      "Epoch 43 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.1610\n",
      "Epoch 43 Batch [90/300]\tTime 0.144 0.145\tLoss 0.0000 0.1563\n",
      "Epoch 43 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.1408\n",
      "Epoch 43 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.1516\n",
      "Epoch 43 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1391\n",
      "Epoch 43 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1376\n",
      "Epoch 43 Batch [140/300]\tTime 0.146 0.145\tLoss 0.0000 0.1302\n",
      "Epoch 43 Batch [150/300]\tTime 0.144 0.145\tLoss 0.0000 0.1219\n",
      "Epoch 43 Batch [160/300]\tTime 0.145 0.145\tLoss 0.0000 0.1143\n",
      "Epoch 43 Batch [170/300]\tTime 0.144 0.145\tLoss 1.2439 0.1159\n",
      "Epoch 43 Batch [180/300]\tTime 0.144 0.145\tLoss 0.0000 0.1095\n",
      "Epoch 43 Batch [190/300]\tTime 0.150 0.145\tLoss 0.0000 0.1037\n",
      "Epoch 43 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.0986\n",
      "Epoch 43 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.0939\n",
      "Epoch 43 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.1054\n",
      "Epoch 43 Batch [230/300]\tTime 0.145 0.145\tLoss 0.2035 0.1017\n",
      "Epoch 43 Batch [240/300]\tTime 0.144 0.145\tLoss 0.9783 0.1091\n",
      "Epoch 43 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1047\n",
      "Epoch 43 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1238\n",
      "Epoch 43 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1192\n",
      "Epoch 43 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.1197\n",
      "Epoch 43 Batch [290/300]\tTime 0.145 0.145\tLoss 0.0000 0.1158\n",
      "Epoch 44 Batch [0/300]\tTime 0.165 0.165\tLoss 0.0000 0.0000\n",
      "Epoch 44 Batch [10/300]\tTime 0.145 0.155\tLoss 1.8187 0.4688\n",
      "Epoch 44 Batch [20/300]\tTime 0.144 0.150\tLoss 0.0000 0.2456\n",
      "Epoch 44 Batch [30/300]\tTime 0.145 0.148\tLoss 0.0000 0.1858\n",
      "Epoch 44 Batch [40/300]\tTime 0.155 0.148\tLoss 0.0000 0.1405\n",
      "Epoch 44 Batch [50/300]\tTime 0.145 0.148\tLoss 0.0000 0.1374\n",
      "Epoch 44 Batch [60/300]\tTime 0.144 0.148\tLoss 0.0563 0.1423\n",
      "Epoch 44 Batch [70/300]\tTime 0.144 0.147\tLoss 0.0000 0.1223\n",
      "Epoch 44 Batch [80/300]\tTime 0.144 0.147\tLoss 0.0000 0.1387\n",
      "Epoch 44 Batch [90/300]\tTime 0.144 0.147\tLoss 0.0000 0.1413\n",
      "Epoch 44 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.1273\n",
      "Epoch 44 Batch [110/300]\tTime 0.144 0.146\tLoss 0.0000 0.1221\n",
      "Epoch 44 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0000 0.1120\n",
      "Epoch 44 Batch [130/300]\tTime 0.144 0.146\tLoss 0.0000 0.1050\n",
      "Epoch 44 Batch [140/300]\tTime 0.145 0.146\tLoss 0.0000 0.1025\n",
      "Epoch 44 Batch [150/300]\tTime 0.144 0.146\tLoss 0.0000 0.0957\n",
      "Epoch 44 Batch [160/300]\tTime 0.144 0.146\tLoss 0.0000 0.0898\n",
      "Epoch 44 Batch [170/300]\tTime 0.144 0.146\tLoss 0.0000 0.0920\n",
      "Epoch 44 Batch [180/300]\tTime 0.145 0.146\tLoss 0.0000 0.0869\n",
      "Epoch 44 Batch [190/300]\tTime 0.144 0.146\tLoss 0.0000 0.0928\n",
      "Epoch 44 Batch [200/300]\tTime 0.149 0.146\tLoss 0.0000 0.0951\n",
      "Epoch 44 Batch [210/300]\tTime 0.145 0.146\tLoss 0.0000 0.1059\n",
      "Epoch 44 Batch [220/300]\tTime 0.146 0.145\tLoss 0.0000 0.1011\n",
      "Epoch 44 Batch [230/300]\tTime 0.145 0.145\tLoss 0.0000 0.1009\n",
      "Epoch 44 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1009\n",
      "Epoch 44 Batch [250/300]\tTime 0.144 0.145\tLoss 3.2558 0.1116\n",
      "Epoch 44 Batch [260/300]\tTime 0.145 0.145\tLoss 0.1293 0.1238\n",
      "Epoch 44 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1193\n",
      "Epoch 44 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.1201\n",
      "Epoch 44 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.1191\n",
      "Epoch 45 Batch [0/300]\tTime 0.154 0.154\tLoss 0.0000 0.0000\n",
      "Epoch 45 Batch [10/300]\tTime 0.145 0.150\tLoss 0.0000 0.4327\n",
      "Epoch 45 Batch [20/300]\tTime 0.145 0.147\tLoss 0.0000 0.5537\n",
      "Epoch 45 Batch [30/300]\tTime 0.144 0.146\tLoss 0.0000 0.4287\n",
      "Epoch 45 Batch [40/300]\tTime 0.145 0.146\tLoss 2.8882 0.4478\n",
      "Epoch 45 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.4170\n",
      "Epoch 45 Batch [60/300]\tTime 0.144 0.146\tLoss 0.0000 0.3608\n",
      "Epoch 45 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.3100\n",
      "Epoch 45 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2717\n",
      "Epoch 45 Batch [90/300]\tTime 0.145 0.145\tLoss 2.8983 0.2737\n",
      "Epoch 45 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2613\n",
      "Epoch 45 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.3397\n",
      "Epoch 45 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.3185\n",
      "Epoch 45 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.2942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45 Batch [140/300]\tTime 0.145 0.145\tLoss 2.4387 0.3040\n",
      "Epoch 45 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.2894\n",
      "Epoch 45 Batch [160/300]\tTime 0.151 0.145\tLoss 0.0000 0.2781\n",
      "Epoch 45 Batch [170/300]\tTime 0.145 0.145\tLoss 0.0000 0.2618\n",
      "Epoch 45 Batch [180/300]\tTime 0.146 0.145\tLoss 0.0000 0.2473\n",
      "Epoch 45 Batch [190/300]\tTime 0.146 0.145\tLoss 0.0000 0.2344\n",
      "Epoch 45 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.2227\n",
      "Epoch 45 Batch [210/300]\tTime 0.145 0.145\tLoss 0.0000 0.2305\n",
      "Epoch 45 Batch [220/300]\tTime 0.145 0.145\tLoss 0.4940 0.2569\n",
      "Epoch 45 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2458\n",
      "Epoch 45 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2356\n",
      "Epoch 45 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.2265\n",
      "Epoch 45 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.2178\n",
      "Epoch 45 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.2123\n",
      "Epoch 45 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.2048\n",
      "Epoch 45 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1978\n",
      "Epoch 46 Batch [0/300]\tTime 0.158 0.158\tLoss 0.0000 0.0000\n",
      "Epoch 46 Batch [10/300]\tTime 0.145 0.151\tLoss 0.0000 0.0000\n",
      "Epoch 46 Batch [20/300]\tTime 0.144 0.148\tLoss 0.0000 0.0057\n",
      "Epoch 46 Batch [30/300]\tTime 0.145 0.147\tLoss 1.3795 0.0855\n",
      "Epoch 46 Batch [40/300]\tTime 0.144 0.146\tLoss 0.2736 0.0976\n",
      "Epoch 46 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.0928\n",
      "Epoch 46 Batch [60/300]\tTime 0.144 0.146\tLoss 1.0623 0.1704\n",
      "Epoch 46 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.1973\n",
      "Epoch 46 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0000 0.2498\n",
      "Epoch 46 Batch [90/300]\tTime 0.145 0.145\tLoss 0.0000 0.2224\n",
      "Epoch 46 Batch [100/300]\tTime 0.144 0.145\tLoss 0.0000 0.2169\n",
      "Epoch 46 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.2065\n",
      "Epoch 46 Batch [120/300]\tTime 0.145 0.145\tLoss 0.0000 0.1902\n",
      "Epoch 46 Batch [130/300]\tTime 0.145 0.145\tLoss 0.0000 0.1826\n",
      "Epoch 46 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1697\n",
      "Epoch 46 Batch [150/300]\tTime 0.144 0.145\tLoss 0.9908 0.1749\n",
      "Epoch 46 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1712\n",
      "Epoch 46 Batch [170/300]\tTime 0.145 0.145\tLoss 0.0000 0.1617\n",
      "Epoch 46 Batch [180/300]\tTime 0.145 0.145\tLoss 0.0000 0.1702\n",
      "Epoch 46 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.1696\n",
      "Epoch 46 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.1666\n",
      "Epoch 46 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1587\n",
      "Epoch 46 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.1516\n",
      "Epoch 46 Batch [230/300]\tTime 0.144 0.145\tLoss 1.7839 0.1527\n",
      "Epoch 46 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1509\n",
      "Epoch 46 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1535\n",
      "Epoch 46 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1510\n",
      "Epoch 46 Batch [270/300]\tTime 0.144 0.145\tLoss 0.0000 0.1454\n",
      "Epoch 46 Batch [280/300]\tTime 0.144 0.145\tLoss 0.0000 0.1445\n",
      "Epoch 46 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.1440\n",
      "Epoch 47 Batch [0/300]\tTime 0.149 0.149\tLoss 3.5580 3.5580\n",
      "Epoch 47 Batch [10/300]\tTime 0.144 0.148\tLoss 0.0000 0.3235\n",
      "Epoch 47 Batch [20/300]\tTime 0.144 0.147\tLoss 0.0000 0.1983\n",
      "Epoch 47 Batch [30/300]\tTime 0.144 0.146\tLoss 0.0000 0.1343\n",
      "Epoch 47 Batch [40/300]\tTime 0.145 0.145\tLoss 0.0000 0.1015\n",
      "Epoch 47 Batch [50/300]\tTime 0.144 0.145\tLoss 0.0000 0.1015\n",
      "Epoch 47 Batch [60/300]\tTime 0.144 0.145\tLoss 0.0000 0.0966\n",
      "Epoch 47 Batch [70/300]\tTime 0.144 0.145\tLoss 0.0000 0.0914\n",
      "Epoch 47 Batch [80/300]\tTime 0.144 0.145\tLoss 0.0875 0.0969\n",
      "Epoch 47 Batch [90/300]\tTime 0.146 0.145\tLoss 0.0000 0.1071\n",
      "Epoch 47 Batch [100/300]\tTime 0.146 0.145\tLoss 1.7526 0.1514\n",
      "Epoch 47 Batch [110/300]\tTime 0.144 0.145\tLoss 0.0000 0.1378\n",
      "Epoch 47 Batch [120/300]\tTime 0.144 0.145\tLoss 0.0000 0.1264\n",
      "Epoch 47 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1514\n",
      "Epoch 47 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1503\n",
      "Epoch 47 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.1427\n",
      "Epoch 47 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1347\n",
      "Epoch 47 Batch [170/300]\tTime 0.144 0.145\tLoss 1.6699 0.1366\n",
      "Epoch 47 Batch [180/300]\tTime 0.145 0.145\tLoss 0.0000 0.1383\n",
      "Epoch 47 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.1311\n",
      "Epoch 47 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1280\n",
      "Epoch 47 Batch [210/300]\tTime 0.145 0.145\tLoss 0.0000 0.1330\n",
      "Epoch 47 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.1334\n",
      "Epoch 47 Batch [230/300]\tTime 0.145 0.145\tLoss 1.4253 0.1532\n",
      "Epoch 47 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.1484\n",
      "Epoch 47 Batch [250/300]\tTime 0.144 0.145\tLoss 1.8516 0.1498\n",
      "Epoch 47 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1498\n",
      "Epoch 47 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.1467\n",
      "Epoch 47 Batch [280/300]\tTime 0.144 0.145\tLoss 0.3657 0.1454\n",
      "Epoch 47 Batch [290/300]\tTime 0.143 0.145\tLoss 0.0000 0.1441\n",
      "Epoch 48 Batch [0/300]\tTime 0.170 0.170\tLoss 0.0000 0.0000\n",
      "Epoch 48 Batch [10/300]\tTime 0.145 0.153\tLoss 0.0000 0.0063\n",
      "Epoch 48 Batch [20/300]\tTime 0.145 0.149\tLoss 0.0000 0.2666\n",
      "Epoch 48 Batch [30/300]\tTime 0.144 0.147\tLoss 0.0000 0.1944\n",
      "Epoch 48 Batch [40/300]\tTime 0.145 0.147\tLoss 0.4325 0.2591\n",
      "Epoch 48 Batch [50/300]\tTime 0.144 0.146\tLoss 0.0000 0.2095\n",
      "Epoch 48 Batch [60/300]\tTime 0.145 0.146\tLoss 0.7676 0.2006\n",
      "Epoch 48 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.2034\n",
      "Epoch 48 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.1851\n",
      "Epoch 48 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.1912\n",
      "Epoch 48 Batch [100/300]\tTime 0.144 0.146\tLoss 0.4689 0.1809\n",
      "Epoch 48 Batch [110/300]\tTime 0.145 0.145\tLoss 0.0000 0.1646\n",
      "Epoch 48 Batch [120/300]\tTime 0.147 0.145\tLoss 0.0000 0.1510\n",
      "Epoch 48 Batch [130/300]\tTime 0.144 0.145\tLoss 0.0000 0.1520\n",
      "Epoch 48 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1412\n",
      "Epoch 48 Batch [150/300]\tTime 0.145 0.145\tLoss 0.0000 0.1437\n",
      "Epoch 48 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1527\n",
      "Epoch 48 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.1541\n",
      "Epoch 48 Batch [180/300]\tTime 0.144 0.145\tLoss 0.9325 0.1530\n",
      "Epoch 48 Batch [190/300]\tTime 0.145 0.145\tLoss 0.0000 0.1573\n",
      "Epoch 48 Batch [200/300]\tTime 0.144 0.145\tLoss 0.0000 0.1518\n",
      "Epoch 48 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.1567\n",
      "Epoch 48 Batch [220/300]\tTime 0.145 0.145\tLoss 0.0000 0.1844\n",
      "Epoch 48 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.1950\n",
      "Epoch 48 Batch [240/300]\tTime 0.145 0.145\tLoss 0.0000 0.1884\n",
      "Epoch 48 Batch [250/300]\tTime 0.144 0.145\tLoss 0.0000 0.1809\n",
      "Epoch 48 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1763\n",
      "Epoch 48 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.1698\n",
      "Epoch 48 Batch [280/300]\tTime 0.144 0.145\tLoss 4.0671 0.1851\n",
      "Epoch 48 Batch [290/300]\tTime 0.144 0.145\tLoss 0.0000 0.1787\n",
      "Epoch 49 Batch [0/300]\tTime 0.175 0.175\tLoss 0.0000 0.0000\n",
      "Epoch 49 Batch [10/300]\tTime 0.144 0.156\tLoss 0.0000 0.0000\n",
      "Epoch 49 Batch [20/300]\tTime 0.144 0.150\tLoss 0.0000 0.0000\n",
      "Epoch 49 Batch [30/300]\tTime 0.144 0.149\tLoss 0.0000 0.0674\n",
      "Epoch 49 Batch [40/300]\tTime 0.144 0.148\tLoss 0.0000 0.1511\n",
      "Epoch 49 Batch [50/300]\tTime 0.144 0.147\tLoss 0.3821 0.1445\n",
      "Epoch 49 Batch [60/300]\tTime 0.145 0.147\tLoss 2.5122 0.2033\n",
      "Epoch 49 Batch [70/300]\tTime 0.144 0.146\tLoss 0.0000 0.1776\n",
      "Epoch 49 Batch [80/300]\tTime 0.144 0.146\tLoss 0.0000 0.1639\n",
      "Epoch 49 Batch [90/300]\tTime 0.144 0.146\tLoss 0.0000 0.1549\n",
      "Epoch 49 Batch [100/300]\tTime 0.144 0.146\tLoss 0.0000 0.1891\n",
      "Epoch 49 Batch [110/300]\tTime 0.144 0.146\tLoss 0.0000 0.1721\n",
      "Epoch 49 Batch [120/300]\tTime 0.144 0.146\tLoss 0.0000 0.1578\n",
      "Epoch 49 Batch [130/300]\tTime 0.145 0.145\tLoss 0.8574 0.1642\n",
      "Epoch 49 Batch [140/300]\tTime 0.144 0.145\tLoss 0.0000 0.1748\n",
      "Epoch 49 Batch [150/300]\tTime 0.145 0.145\tLoss 0.6739 0.1861\n",
      "Epoch 49 Batch [160/300]\tTime 0.144 0.145\tLoss 0.0000 0.1745\n",
      "Epoch 49 Batch [170/300]\tTime 0.144 0.145\tLoss 0.0000 0.2026\n",
      "Epoch 49 Batch [180/300]\tTime 0.145 0.145\tLoss 0.8971 0.2041\n",
      "Epoch 49 Batch [190/300]\tTime 0.144 0.145\tLoss 0.0000 0.2001\n",
      "Epoch 49 Batch [200/300]\tTime 0.145 0.145\tLoss 0.0000 0.2184\n",
      "Epoch 49 Batch [210/300]\tTime 0.144 0.145\tLoss 0.0000 0.2333\n",
      "Epoch 49 Batch [220/300]\tTime 0.144 0.145\tLoss 0.0000 0.2227\n",
      "Epoch 49 Batch [230/300]\tTime 0.144 0.145\tLoss 0.0000 0.2131\n",
      "Epoch 49 Batch [240/300]\tTime 0.144 0.145\tLoss 0.0000 0.2042\n",
      "Epoch 49 Batch [250/300]\tTime 0.145 0.145\tLoss 0.0000 0.1980\n",
      "Epoch 49 Batch [260/300]\tTime 0.144 0.145\tLoss 0.0000 0.1950\n",
      "Epoch 49 Batch [270/300]\tTime 0.145 0.145\tLoss 0.0000 0.1878\n",
      "Epoch 49 Batch [280/300]\tTime 0.145 0.145\tLoss 0.0000 0.1872\n",
      "Epoch 49 Batch [290/300]\tTime 0.144 0.145\tLoss 0.7483 0.1957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting embedding from the provided model ...\n",
      "Batch 0\n",
      "Embedding extraction is done.\n",
      "[0, 2000, 0, 3000, 0]\n",
      "Accuracy 0.9278947368421052\n",
      "[0, 1900, 0, 3100, 0]\n",
      "Accuracy 0.9327777777777778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1800, 0, 3200, 0]\n",
      "Accuracy 0.9370588235294117\n",
      "[0, 1700, 0, 3300, 0]\n",
      "Accuracy 0.92875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1600, 0, 3400, 0]\n",
      "Accuracy 0.9306666666666666\n",
      "[0, 1500, 0, 3500, 0]\n",
      "Accuracy 0.9271428571428572\n",
      "[0, 1400, 0, 3600, 0]\n",
      "Accuracy 0.9307692307692308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1300, 0, 3700, 0]\n",
      "Accuracy 0.9225\n",
      "[0, 1200, 0, 3800, 0]\n",
      "Accuracy 0.9181818181818182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1100, 0, 3900, 0]\n",
      "Accuracy 0.922\n"
     ]
    }
   ],
   "source": [
    "numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "while numLabeled < 4000:\n",
    "    print([len(x) for x in dataset.set_indices])\n",
    "#     sys.stdout.flush()\n",
    "\n",
    "    # Get indices of samples to get user to label\n",
    "    if numLabeled == 0:\n",
    "        indices = np.random.choice(dataset.current_set, kwargs[\"N\"], replace=False).tolist()\n",
    "    else:\n",
    "        indices = sampler.select_batch(**kwargs)\n",
    "    moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, indices)\n",
    "    numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "\n",
    "    # Train on samples that have been labeled so far\n",
    "    dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "    X_train = dataset.em[dataset.current_set]\n",
    "    y_train = np.asarray(dataset.getlabels())\n",
    "\n",
    "    kwargs[\"model\"].fit(X_train, y_train)\n",
    "    joblib.dump(kwargs[\"model\"], \"%s/%s_%04d.skmodel\"%(notebook_output_dirname, 'original_embedding_classifier', numLabeled))\n",
    "\n",
    "    # Test on the samples that have not been labeled\n",
    "    dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "    dataset.embedding_mode()\n",
    "    X_test = dataset.em[dataset.current_set]\n",
    "    y_test = np.asarray(dataset.getlabels())\n",
    "    print(\"Accuracy\", kwargs[\"model\"].score(X_test, y_test))\n",
    "    accuracies_finetuned_embedding.append(kwargs[\"model\"].score(X_test, y_test))\n",
    "\n",
    "    sys.stdout.flush()\n",
    "    if numLabeled % 2000 == 1000:\n",
    "        dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "        finetune_embedding(model, checkpoint['loss_type'], dataset, 10, 4, 100 if numLabeled == 1000 else 50)\n",
    "        save_checkpoint({\n",
    "        'arch': checkpoint['arch'],\n",
    "        'state_dict': model.state_dict(),\n",
    "        #'optimizer' : optimizer.state_dict(),\n",
    "        'loss_type' : checkpoint['loss_type'],\n",
    "        'feat_dim' : checkpoint['feat_dim'],\n",
    "        'num_classes' : 21\n",
    "        }, False, \"%s/%s%s_%s_%04d.tar\"%(notebook_output_dirname, 'finetuned', checkpoint['loss_type'], checkpoint['arch'], numLabeled))\n",
    "\n",
    "        dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "        dataset.updateEmbedding(model)\n",
    "        dataset.embedding_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AL_trace = {}\n",
    "# AL_trace['num_labeled'] = [i for i in range(1000,4100,100)]\n",
    "# AL_trace['accuracies'] = accuracies_finetuned_embedding\n",
    "# with open('%s/original_embedding_classifier_AL_trace.pkl'%(notebook_output_dirname), 'wb') as o:\n",
    "#     pickle.dump(AL_trace, o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_trace = pickle.load(open('%s/finetune_embedding_before_AL/finetuned_embedding_classifier_AL_trace.pkl'%(notebook_output_dirname), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:2: MatplotlibDeprecationWarning: pyplot.hold is deprecated.\n",
      "    Future behavior will be consistent with the long-time default:\n",
      "    plot commands add elements without first clearing the\n",
      "    Axes and/or Figure.\n",
      "  from ipykernel import kernelapp as app\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/__init__.py:911: MatplotlibDeprecationWarning: axes.hold is deprecated. Please remove it from your matplotlibrc and/or style files.\n",
      "  mplDeprecation)\n",
      "/data/anaconda/envs/py35/lib/python3.5/site-packages/matplotlib/rcsetup.py:156: MatplotlibDeprecationWarning: axes.hold is deprecated, will be removed in 3.0\n",
      "  mplDeprecation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fb411a71748>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(finetuned_trace['num_labeled'], finetuned_trace['accuracies'], 'g', label='Retrain embedding before AL')\n",
    "plt.hold(True)\n",
    "plt.plot(AL_trace['num_labeled'], AL_trace['accuracies'], 'r', label='Using original embedding')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.62575,\n",
       " 0.6330769230769231,\n",
       " 0.6521052631578947,\n",
       " 0.6667567567567567,\n",
       " 0.6766666666666666,\n",
       " 0.6885714285714286,\n",
       " 0.6932352941176471,\n",
       " 0.7233333333333334,\n",
       " 0.729375,\n",
       " 0.7383870967741936,\n",
       " 0.7613333333333333,\n",
       " 0.7655172413793103,\n",
       " 0.7882142857142858,\n",
       " 0.7992592592592592,\n",
       " 0.8026923076923077,\n",
       " 0.8208,\n",
       " 0.8325,\n",
       " 0.8456521739130435,\n",
       " 0.8622727272727273,\n",
       " 0.8714285714285714,\n",
       " 0.8835,\n",
       " 0.9278947368421052,\n",
       " 0.9327777777777778,\n",
       " 0.9370588235294117,\n",
       " 0.92875,\n",
       " 0.9306666666666666,\n",
       " 0.9271428571428572,\n",
       " 0.9307692307692308,\n",
       " 0.9225,\n",
       " 0.9181818181818182,\n",
       " 0.922]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AL_trace['accuracies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Guess Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on the samples that have not been labeled\n",
    "y_test = np.asarray(dataset.getlabels())\n",
    "number_of_classes = 21\n",
    "class_proportions = dict.fromkeys(range(number_of_classes))\n",
    "for c in range(number_of_classes):\n",
    "    c_count = len([l for l in y_test if l == c])\n",
    "    class_proportions[c] = float(c_count)/len(y_test)\n",
    "class_proportions_csum = np.cumsum([v for k,v in class_proportions.items()])\n",
    "\n",
    "fraction_correct_uniform = []\n",
    "fraction_correct_proportional = []\n",
    "\n",
    "for run in tqdm.tqdm(range(100)):\n",
    "    num_correct_uniform = 0\n",
    "    num_correct_proportional = 0\n",
    "    for i in range(len(y_test)):\n",
    "        clf_guess_uniform = np.random.randint(0, number_of_classes)\n",
    "        coin = np.random.rand()\n",
    "        clf_guess_proportional = next(i for i in range(len(class_proportions_csum)) if (coin>class_proportions_csum[i]) == False)\n",
    "        if clf_guess_uniform == y_test[i]:\n",
    "            num_correct_uniform += 1\n",
    "        if clf_guess_proportional == y_test[i]:\n",
    "            num_correct_proportional += 1\n",
    "    fraction_correct_uniform.append(float(num_correct_uniform)/len(y_test))\n",
    "    fraction_correct_proportional.append(float(num_correct_proportional)/len(y_test))\n",
    "print(np.mean(fraction_correct_uniform))\n",
    "print(np.mean(fraction_correct_proportional))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_emb_dirname = './temp_output'\n",
    "#os.makedirs(model_emb_dirname, exist_ok=True)\n",
    "#plot_embedding_images(dataset.em[:], np.asarray(dataset.getlabels()) , dataset.getpaths(), {}, model_emb_dirname+'/embedding_plot.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_idx = np.random.randint(len(dataset.samples))\n",
    "query_img = dataset.loader(imagepaths[query_idx].split('.')[0])\n",
    "query_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # # # IMAGES IN THE SAME SEQUENCE # # # #\n",
    "matching_image_entries = (Image\n",
    "                        .select(Image.seq_id, Image.seq_num_frames, Image.frame_num)\n",
    "                        .where((Image.file_name == imagepaths[query_idx])))\n",
    "mie = matching_image_entries.get()\n",
    "if mie.seq_num_frames > 1:\n",
    "    images_in_seq = (Image\n",
    "                    .select(Image.file_name)\n",
    "                    .where((Image.seq_id == mie.seq_id) & (Image.file_name << imagepaths))\n",
    "                    )\n",
    "images_in_seq = sorted(list(set([i.file_name for i in images_in_seq])))\n",
    "seq_img_idx = [imagepaths.index(im) for im in images_in_seq]\n",
    "plt.figure(figsize = (20, 5))\n",
    "for i in range(len(seq_img_idx)):\n",
    "#     if images_in_seq[i] != imagepaths[query_idx]:\n",
    "    img = dataset.loader(images_in_seq[i].split('.')[0])\n",
    "    plt.subplot(1, len(seq_img_idx), i+1)\n",
    "    plt.imshow(img)\n",
    "#         img.save('same_seq_img%d.png'%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # # CLOSEST IN (EMBEDDING) FEATURE SPACE # # # #\n",
    "timer = time.time()\n",
    "nbrs = NearestNeighbors(n_neighbors=len(seq_img_idx)).fit(dataset.em)\n",
    "print('Finished fitting nearest neighbors for whole dataset in %0.2f seconds'%(float(time.time() - timer)))\n",
    "distances, indices = nbrs.kneighbors(dataset.em[query_idx:query_idx+1])\n",
    "query_nbrs_indices = indices[0, 0:len(seq_img_idx)]\n",
    "print(query_nbrs_indices)\n",
    "plt.figure(figsize = (20, 5))\n",
    "for i in range(len(query_nbrs_indices)):\n",
    "    nbr_idx = query_nbrs_indices[i]\n",
    "    nbr_img = dataset.loader(imagepaths[nbr_idx].split('.')[0])\n",
    "    plt.subplot(1, len(seq_img_idx), i+1)\n",
    "    plt.imshow(nbr_img)\n",
    "#     nbr_img.save(\"embedding_nnbr_img%d.png\"%i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_common_images = len(set(seq_img_idx).intersection(set(query_nbrs_indices)))\n",
    "number_of_images_in_seq = len(seq_img_idx)\n",
    "print(float(number_of_common_images)/number_of_images_in_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([len(x) for x in dataset.set_indices])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Classifier using a Basic Active Learning Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_STRATEGY = 'confidence'\n",
    "ACTIVE_LEARNING_BUDGET = 500\n",
    "sampler = get_AL_sampler(AL_STRATEGY)(dataset.em, dataset.getalllabels(), 1234)\n",
    "\n",
    "kwargs = {}\n",
    "kwargs[\"N\"] = 5 # number of samples to label between classifier retraining\n",
    "kwargs[\"already_selected\"] = dataset.set_indices[DetectionKind.UserDetection.value]\n",
    "kwargs[\"model\"] = MLPClassifier(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start the active learning loop\")\n",
    "numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "num_labeled_samples = []\n",
    "accuracy = []\n",
    "while numLabeled < ACTIVE_LEARNING_BUDGET:\n",
    "    if numLabeled == 0:\n",
    "        indices = np.random.choice(dataset.current_set, kwargs[\"N\"], replace=False).tolist()\n",
    "    else:\n",
    "        indices = sampler.select_batch(**kwargs)\n",
    "    moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, indices)\n",
    "    numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    \n",
    "    # Train on samples that have been labeled so far\n",
    "    dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "    X_train = dataset.em[dataset.current_set]\n",
    "    y_train = np.asarray(dataset.getlabels())\n",
    "\n",
    "    kwargs[\"model\"].fit(X_train, y_train)\n",
    "    \n",
    "    # Test on the samples that have not been labeled\n",
    "    dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "    dataset.embedding_mode()\n",
    "    X_test = dataset.em[dataset.current_set]\n",
    "    y_test = np.asarray(dataset.getlabels())\n",
    "    \n",
    "    # Store performance to plot\n",
    "    num_labeled_samples.append(numLabeled)\n",
    "    accuracy.append(kwargs[\"model\"].score(X_test, y_test))\n",
    "print(\"Finished the active learning loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_labeled_samples, accuracy)\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('MLP Classifier Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_anchor_idx = np.random.randint(len(dataset))\n",
    "random_anchor_img = dataset.loader(imagepaths[random_anchor_idx].split('.JPG')[0])\n",
    "plt.imshow(random_anchor_img)\n",
    "plt.title(labels[random_anchor_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest Neighbors in Embedding Space\n",
    "Every time the model provides an example that it is uncertain about, label that sample along with its $k$ nearest neighbors in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer = time.time()\n",
    "nbrs = NearestNeighbors(n_neighbors=11).fit(dataset.em)\n",
    "print('Finished fitting nearest neighbors for whole dataset in %0.2f seconds'%(float(time.time() - timer)))\n",
    "distances, indices = nbrs.kneighbors(dataset.em)\n",
    "ten_closest_to_anchor = indices[random_anchor_idx, 1:11]\n",
    "\n",
    "# show the images\n",
    "plt.figure(figsize=(20,10))\n",
    "for i in range(len(ten_closest_to_anchor)):\n",
    "    nbr_img = dataset.loader(imagepaths[ten_closest_to_anchor[i]].split('.')[0])\n",
    "    plt.subplot(2, 5, i+1)\n",
    "    plt.imshow(nbr_img)\n",
    "    plt.title(labels[ten_closest_to_anchor[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classifier using Basic AL Strategy + NNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_db = PostgresqlDatabase(DB_NAME, user=DB_USER, password=DB_PASSWORD, host='localhost')\n",
    "target_db.connect(reuse_if_open=True)\n",
    "db_proxy.initialize(target_db)\n",
    "dataset_query = Detection.select(Detection.image_id, Oracle.label, Detection.kind).join(Oracle).limit(DB_LIMIT)\n",
    "dataset = SQLDataLoader(CROP_DIR, query=dataset_query, is_training=False, kind=DetectionKind.ModelDetection.value, num_workers=8, limit=DB_LIMIT)\n",
    "dataset.updateEmbedding(model)\n",
    "sample_ids = [s[0] for s in dataset.samples]\n",
    "labels = [s[1] for s in dataset.samples]\n",
    "imagepaths = dataset.getallpaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_STRATEGY = 'confidence'\n",
    "ACTIVE_LEARNING_BUDGET = 500\n",
    "sampler = get_AL_sampler(AL_STRATEGY)(dataset.em, dataset.getalllabels(), 1234)\n",
    "\n",
    "kwargs = {}\n",
    "kwargs[\"N\"] = 1 # number of samples to label between classifier retraining\n",
    "kwargs[\"already_selected\"] = dataset.set_indices[DetectionKind.UserDetection.value]\n",
    "kwargs[\"model\"] = MLPClassifier(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start the active learning loop\")\n",
    "numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "num_labeled_samples_2 = []\n",
    "accuracy_2 = []\n",
    "while numLabeled < ACTIVE_LEARNING_BUDGET:\n",
    "    if numLabeled == 0:\n",
    "        indices = np.random.choice(dataset.current_set, 5, replace=False).tolist()\n",
    "        moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, indices)\n",
    "        numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    else:\n",
    "        indices = sampler.select_batch(**kwargs)\n",
    "        moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, indices)\n",
    "        numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    \n",
    "        # Label additional indices based on KNN\n",
    "        nbrs = NearestNeighbors(n_neighbors=len(dataset.samples)).fit(dataset.em)\n",
    "        distances, knnindices = nbrs.kneighbors(dataset.em)\n",
    "        four_closest_to_anchor = []\n",
    "        closest_to_anchor = knnindices[indices[0], 1:-1]\n",
    "        for p in range(len(closest_to_anchor)):\n",
    "            if closest_to_anchor[p] in dataset.set_indices[DetectionKind.ModelDetection.value]:\n",
    "                four_closest_to_anchor.append(closest_to_anchor[p])\n",
    "\n",
    "            if len(four_closest_to_anchor) == 4:\n",
    "                break\n",
    "        moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, four_closest_to_anchor)\n",
    "        numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    \n",
    "    # Train on samples that have been labeled so far\n",
    "    dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "    X_train = dataset.em[dataset.current_set]\n",
    "    y_train = np.asarray(dataset.getlabels())\n",
    "\n",
    "    kwargs[\"model\"].fit(X_train, y_train)\n",
    "    \n",
    "    # Test on the samples that have not been labeled\n",
    "    dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "    dataset.embedding_mode()\n",
    "    X_test = dataset.em[dataset.current_set]\n",
    "    y_test = np.asarray(dataset.getlabels())\n",
    "    \n",
    "    # Store performance to plot\n",
    "    num_labeled_samples_2.append(numLabeled)\n",
    "    accuracy_2.append(kwargs[\"model\"].score(X_test, y_test))\n",
    "print(\"Finished the active learning loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_labeled_samples_2, accuracy_2)\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('MLP Classifier Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_labeled_samples, accuracy, 'g', label='Confidence Sampling')\n",
    "plt.hold(True)\n",
    "plt.plot(num_labeled_samples_2, accuracy_2, 'b', label='Confidence + Embedding KNN')\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('MLP Classifier Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same Pair of Classes With Highest Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_probs = kwargs[\"model\"].predict_proba(dataset.em)\n",
    "two_highest_prob_classes = set((-class_probs[random_anchor_idx]).argsort()[0:2])\n",
    "print(two_highest_prob_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "same_pair = []\n",
    "for idx in range(len(class_probs)):\n",
    "    if idx != random_anchor_idx:\n",
    "        two_highest = set((-class_probs[idx]).argsort()[0:2])\n",
    "        if two_highest == two_highest_prob_classes:\n",
    "            same_pair.append(idx)\n",
    "print(same_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "for i in range(10):\n",
    "    img = dataset.loader(imagepaths[same_pair[i]].split('.JPG')[0])\n",
    "    plt.subplot(2,5,i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(labels[same_pair[i]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Classifier using Basic AL Strategy + Class Pair Confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_db = PostgresqlDatabase(DB_NAME, user=DB_USER, password=DB_PASSWORD, host='localhost')\n",
    "target_db.connect(reuse_if_open=True)\n",
    "db_proxy.initialize(target_db)\n",
    "dataset_query = Detection.select(Detection.image_id, Oracle.label, Detection.kind).join(Oracle).limit(DB_LIMIT)\n",
    "dataset = SQLDataLoader(CROP_DIR, query=dataset_query, is_training=False, kind=DetectionKind.ModelDetection.value, num_workers=8, limit=DB_LIMIT)\n",
    "dataset.updateEmbedding(model)\n",
    "sample_ids = [s[0] for s in dataset.samples]\n",
    "labels = [s[1] for s in dataset.samples]\n",
    "imagepaths = dataset.getallpaths()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_STRATEGY = 'confidence'\n",
    "ACTIVE_LEARNING_BUDGET = 500\n",
    "sampler = get_AL_sampler(AL_STRATEGY)(dataset.em, dataset.getalllabels(), 1234)\n",
    "\n",
    "kwargs = {}\n",
    "kwargs[\"N\"] = 1 # number of samples to label between classifier retraining\n",
    "kwargs[\"already_selected\"] = dataset.set_indices[DetectionKind.UserDetection.value]\n",
    "kwargs[\"model\"] = MLPClassifier(alpha=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start the active learning loop\")\n",
    "numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "num_labeled_samples_3 = []\n",
    "accuracy_3 = []\n",
    "while numLabeled < ACTIVE_LEARNING_BUDGET:\n",
    "    if numLabeled == 0:\n",
    "        indices = np.random.choice(dataset.current_set, 5, replace=False).tolist()\n",
    "        moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, indices)\n",
    "        numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    else:\n",
    "        indices = sampler.select_batch(**kwargs)\n",
    "        moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, indices)\n",
    "        numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    \n",
    "        # Label additional indices based class confusion\n",
    "        class_probs = kwargs[\"model\"].predict_proba(dataset.em)\n",
    "        two_highest_prob_classes = set((-class_probs[indices[0]]).argsort()[0:2])\n",
    "\n",
    "        four_with_same_pair = []\n",
    "        for idx in range(len(class_probs)):\n",
    "            if idx != indices[0]:\n",
    "                two_highest = set((-class_probs[idx]).argsort()[0:2])\n",
    "                if (two_highest == two_highest_prob_classes) and (idx in dataset.set_indices[DetectionKind.ModelDetection.value]):\n",
    "                    four_with_same_pair.append(idx)\n",
    "\n",
    "            if len(four_with_same_pair) == 4:\n",
    "                break\n",
    "        moveRecords(dataset, DetectionKind.ModelDetection.value, DetectionKind.UserDetection.value, four_with_same_pair)\n",
    "        numLabeled = len(dataset.set_indices[DetectionKind.UserDetection.value])\n",
    "    \n",
    "    # Train on samples that have been labeled so far\n",
    "    dataset.set_kind(DetectionKind.UserDetection.value)\n",
    "    X_train = dataset.em[dataset.current_set]\n",
    "    y_train = np.asarray(dataset.getlabels())\n",
    "\n",
    "    kwargs[\"model\"].fit(X_train, y_train)\n",
    "    \n",
    "    # Test on the samples that have not been labeled\n",
    "    dataset.set_kind(DetectionKind.ModelDetection.value)\n",
    "    dataset.embedding_mode()\n",
    "    X_test = dataset.em[dataset.current_set]\n",
    "    y_test = np.asarray(dataset.getlabels())\n",
    "    \n",
    "    # Store performance to plot\n",
    "    num_labeled_samples_3.append(numLabeled)\n",
    "    accuracy_3.append(kwargs[\"model\"].score(X_test, y_test))\n",
    "print(\"Finished the active learning loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(num_labeled_samples, accuracy, 'g', label='Confidence Sampling')\n",
    "plt.hold(True)\n",
    "plt.plot(num_labeled_samples_3, accuracy_3, 'b', label='Confidence + Class Pair Confusion')\n",
    "plt.xlabel('Number of Labeled Samples')\n",
    "plt.ylabel('MLP Classifier Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
